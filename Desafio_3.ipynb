{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesphamm/procesamiento_lenguaje_natural/blob/main/Desafio_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AlMslvo7-ZT"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Desaf√≠o 3: Modelo de lenguaje con tokenizaci√≥n por caracteres\n",
        "\n",
        "**Alumna:** Carla Esp√≠nola Hamm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FtxaxD17-ZU",
        "outputId": "3dc85d0d-c476-4a37-ee57-427995595557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "Keras version: 3.10.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "import bs4 as bs\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "# Configuraci√≥n de estilo para gr√°ficos\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3ZhDXkQ7-ZU",
        "outputId": "8e1c4ccf-0476-4dcd-83b7-aaee78ddd289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU detectada: 1 dispositivo(s)\n",
            "   ‚Ä¢ /physical_device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n de GPU\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"\\nGPU detectada: {len(gpus)} dispositivo(s)\")\n",
        "    for gpu in gpus:\n",
        "        print(f\"   ‚Ä¢ {gpu.name}\")\n",
        "\n",
        "    # Habilitar crecimiento din√°mico de memoria\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    # Habilitar mixed precision para acelerar entrenamiento\n",
        "    #tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    #print(\"\\n Mixed Precision (float16) ACTIVADO\")\n",
        "    #print(\"Optimizaciones de GPU activadas\")\n",
        "else:\n",
        "    print(\"\\nNo se detect√≥ GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm9ZxoYW7-ZV"
      },
      "source": [
        "## 1. Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYe9POPP7-ZV",
        "outputId": "d8f26817-5999-406e-fb6f-625596a18f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud total del corpus: 2,077,831 caracteres\n",
            "\n",
            "Primeros 500 caracteres del corpus:\n",
            "--------------------------------------------------\n",
            " yo, juan gallo de andrada, escribano de c√°mara del rey nuestro se√±or, de\n",
            "los que residen en su consejo, certifico y doy fe que, habiendo visto por\n",
            "los se√±ores d√©l un libro intitulado el ingenioso hidalgo de la mancha,\n",
            "compuesto por miguel de cervantes saavedra, tasaron cada pliego del dicho\n",
            "libro a tres maraved√≠s y medio; el cual tiene ochenta y tres pliegos, que\n",
            "al dicho precio monta el dicho libro docientos y noventa maraved√≠s y medio,\n",
            "en que se ha de vender en papel; y dieron licencia para q\n"
          ]
        }
      ],
      "source": [
        "url = 'https://www.textos.info/miguel-de-cervantes-saavedra/el-ingenioso-hidalgo-don-quijote-de-la-mancha/ebook'\n",
        "#url = 'https://www.textos.info/homero/odisea/ebook'\n",
        "raw_html = urllib.request.urlopen(url)\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear art√≠culo. 'lxml' es el parser a utilizar\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Encontrar todos los p√°rrafos del HTML (bajo el tag <p>)\n",
        "# y tenerlos disponible como lista\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "# Concatenar el texto de todos los p√°rrafos\n",
        "corpus = ''\n",
        "for para in article_paragraphs:\n",
        "    corpus += para.text + ' '\n",
        "\n",
        "# Pasar todo el texto a min√∫scula\n",
        "corpus = corpus.lower()\n",
        "\n",
        "print(f\"Longitud total del corpus: {len(corpus):,} caracteres\")\n",
        "print(f\"\\nPrimeros 500 caracteres del corpus:\")\n",
        "print(\"-\" * 50)\n",
        "print(corpus[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npc1wJn07-ZV"
      },
      "source": [
        "## 2. Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WyTfNtk87-ZW"
      },
      "outputs": [],
      "source": [
        "# Definir tama√±o de contexto\n",
        "MAX_CONTEXT_SIZE = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9y4GHbx7-ZV",
        "outputId": "36c931f7-f8e1-4cbb-f79a-ec265620b611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tama√±o del vocabulario: 65 caracteres √∫nicos\n",
            "\n",
            "Caracteres en el vocabulario:\n",
            "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¬°', '¬´', '¬ª', '¬ø', '√†', '√°', '√©', '√≠', '√Ø', '√±', '√≥', '√π', '√∫', '√º', '‚Äî']\n"
          ]
        }
      ],
      "source": [
        "# Crear vocabulario de caracteres √∫nicos\n",
        "chars_vocab = sorted(set(corpus))\n",
        "vocab_size = len(chars_vocab)\n",
        "\n",
        "print(f\"Tama√±o del vocabulario: {vocab_size} caracteres √∫nicos\")\n",
        "print(f\"\\nCaracteres en el vocabulario:\")\n",
        "print(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5EuT7Yj7-ZW",
        "outputId": "904a0bab-3198-443a-9a4e-dec29f7b8940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Ejemplos de mapeo char2idx:\n",
            "  'a' -> 25\n",
            "  'e' -> 29\n",
            "  'i' -> 33\n",
            "  'o' -> 38\n",
            "  'u' -> 44\n",
            "  ' ' -> 2\n",
            "  '.' -> 10\n"
          ]
        }
      ],
      "source": [
        "# Construimos los dicionarios que asignan √≠ndices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servir√° como tokenizador.\n",
        "char2idx = {ch: idx for idx, ch in enumerate(chars_vocab)}\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "\n",
        "print(\"üîó Ejemplos de mapeo char2idx:\")\n",
        "for ch in ['a', 'e', 'i', 'o', 'u', ' ', '.']:\n",
        "    if ch in char2idx:\n",
        "        print(f\"  '{ch}' -> {char2idx[ch]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7APmDFX7-ZW",
        "outputId": "bc4923c0-9435-49f4-a180-5de43a2565a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus tokenizado - shape: (2077831,)\n",
            "\n",
            "Primeros 50 tokens:\n",
            "[ 2 48 38  8  2 34 44 25 37  2 31 25 35 35 38  2 28 29  2 25 37 28 41 25\n",
            " 28 25  8  2 29 42 27 41 33 26 25 37 38  2 28 29  2 27 55 36 25 41 25  2\n",
            " 28 29]\n"
          ]
        }
      ],
      "source": [
        "# Tokenizar el corpus completo\n",
        "tokenized_corpus = np.array([char2idx[ch] for ch in corpus], dtype=np.int32)\n",
        "\n",
        "print(f\"Corpus tokenizado - shape: {tokenized_corpus.shape}\")\n",
        "print(f\"\\nPrimeros 50 tokens:\")\n",
        "print(tokenized_corpus[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvZX6DyL7-ZW"
      },
      "source": [
        "### Estructuraci√≥n del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFeBoZqL7-ZW",
        "outputId": "6dd60065-94e0-4c33-931b-850fd0892f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Divisi√≥n del corpus:\n",
            "  ‚Ä¢ Entrenamiento: 1,870,047 caracteres (90.0%)\n",
            "  ‚Ä¢ Validaci√≥n: 207,784 caracteres (10.0%)\n"
          ]
        }
      ],
      "source": [
        "# separaremos el dataset entre entrenamiento y validaci√≥n.\n",
        "# `p_val` ser√° la proporci√≥n del corpus que se reservar√° para validaci√≥n\n",
        "# `num_val` es la cantidad de secuencias de tama√±o `MAX_CONTEXT_SIZE` que se usar√° en validaci√≥n\n",
        "p_val = 0.1\n",
        "#num_val = int(np.ceil(len(tokenized_corpus)*p_val/MAX_CONTEXT_SIZE))\n",
        "split_idx = int(len(tokenized_corpus) * (1 - p_val))\n",
        "\n",
        "# separamos la porci√≥n de texto utilizada en entrenamiento de la de validaci√≥n.\n",
        "train_corpus = tokenized_corpus[:split_idx]\n",
        "val_corpus = tokenized_corpus[split_idx:]\n",
        "\n",
        "print(f\"üìä Divisi√≥n del corpus:\")\n",
        "print(f\"  ‚Ä¢ Entrenamiento: {len(train_corpus):,} caracteres ({len(train_corpus)/len(tokenized_corpus)*100:.1f}%)\")\n",
        "print(f\"  ‚Ä¢ Validaci√≥n: {len(val_corpus):,} caracteres ({len(val_corpus)/len(tokenized_corpus)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fYTKAUw7-ZW",
        "outputId": "6299e6f8-c8a6-4a90-b278-649ffd3d3da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secuencias de entrenamiento: X=(1869947, 100), y=(1869947, 100)\n",
            "Secuencias de validaci√≥n: X=(207684, 100), y=(207684, 100)\n"
          ]
        }
      ],
      "source": [
        "def create_sequences(corpus_data, seq_length):\n",
        "    \"\"\"Crea secuencias de entrada y target para entrenamiento.\"\"\"\n",
        "    n_sequences = len(corpus_data) - seq_length\n",
        "    X = np.zeros((n_sequences, seq_length), dtype=np.int32)\n",
        "    y = np.zeros((n_sequences, seq_length), dtype=np.int32)\n",
        "    for i in range(n_sequences):\n",
        "        X[i] = corpus_data[i:i + seq_length]\n",
        "        y[i] = corpus_data[i + 1:i + seq_length + 1]\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = create_sequences(train_corpus, MAX_CONTEXT_SIZE)\n",
        "X_val, y_val = create_sequences(val_corpus, MAX_CONTEXT_SIZE)\n",
        "\n",
        "print(f\"Secuencias de entrenamiento: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Secuencias de validaci√≥n: X={X_val.shape}, y={y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaGHZH5C7-ZW",
        "outputId": "42c2985f-20be-484b-e4da-e1a4f3785634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets creados:\n",
            "  ‚Ä¢ Batches de entrenamiento: 1826\n",
            "  ‚Ä¢ Secuencias de validaci√≥n para PPL: 2077\n",
            "  ‚Ä¢ Tama√±o de batch: 1024\n"
          ]
        }
      ],
      "source": [
        "# Crear tf.data.Dataset para entrenamiento\n",
        "BATCH_SIZE = 1024 if gpus else 128\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Crear secuencias de validaci√≥n tokenizadas para PplCallback\n",
        "num_val_sequences = len(val_corpus) // MAX_CONTEXT_SIZE\n",
        "tokenized_sentences_val = [\n",
        "    list(val_corpus[i * MAX_CONTEXT_SIZE:(i + 1) * MAX_CONTEXT_SIZE])\n",
        "    for i in range(num_val_sequences)\n",
        "]\n",
        "\n",
        "print(f\"Datasets creados:\")\n",
        "print(f\"  ‚Ä¢ Batches de entrenamiento: {len(train_dataset)}\")\n",
        "print(f\"  ‚Ä¢ Secuencias de validaci√≥n para PPL: {len(tokenized_sentences_val)}\")\n",
        "print(f\"  ‚Ä¢ Tama√±o de batch: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsNFRRJV7-ZW"
      },
      "source": [
        "## 3. Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prob√© embeddings como representaci√≥n de los tokens, pero al no requerir aprendizaje, el tiempo de entrenamiento con one-hot encodding fue mucho m√°s r√°pido y dado que  el vocabulario es peque√±o (65 caracteres), se puede generar una representaci√≥n directa sin p√©rdida de informaci√≥n. Para vocabularios grandes, los embeddings ser√≠an preferibles para reducir dimensionalidad.\n",
        "\n",
        "Creo n capas de RNN buscando aumentar la capacidad de la red.\n",
        "\n",
        "Uso dropuot para prevenir el overfitting que es bsantante posible en un corpus tan peque√±o y normalizaci√≥n para estabilizar el entrenamiento y acelerar la convergencia."
      ],
      "metadata": {
        "id": "1WzHwWXbRyLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funci√≥n para arquitecturas"
      ],
      "metadata": {
        "id": "qczx9txlcMdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3soqeiQj7-ZW"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding\n",
        "\n",
        "def build_char_language_model(vocab_size, hidden_size=256, num_layers=2,\n",
        "                               rnn_type='lstm', dropout=0.5, embedding_dim=128,\n",
        "                               embed_dropout=0.2, train_type='one-hot'):\n",
        "    \"\"\"Construye un modelo de lenguaje a nivel de caracteres.\"\"\"\n",
        "    rnn_classes = {\n",
        "        'rnn': layers.SimpleRNN,\n",
        "        'lstm': layers.LSTM,\n",
        "        'gru': layers.GRU\n",
        "    }\n",
        "\n",
        "    if rnn_type.lower() not in rnn_classes:\n",
        "        raise ValueError(f\"rnn_type debe ser 'rnn', 'lstm' o 'gru'\")\n",
        "\n",
        "    RNNLayer = rnn_classes[rnn_type.lower()]\n",
        "\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    if train_type.lower() == 'one-hot':\n",
        "      # Reshape para CategoryEncoding: (batch, seq) -> (batch, seq, 1)\n",
        "      x = layers.Reshape((-1, 1))(inputs)\n",
        "      x = TimeDistributed(\n",
        "          CategoryEncoding(num_tokens=vocab_size, output_mode=\"one_hot\")\n",
        "      )(x)\n",
        "    else: ## Embeddings\n",
        "      x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "      x = layers.Dropout(embed_dropout)(x)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      x = RNNLayer(\n",
        "          hidden_size,\n",
        "          return_sequences=True,\n",
        "          dropout=dropout if i < num_layers - 1 else 0,\n",
        "          recurrent_dropout=dropout if i < num_layers - 1 else 0\n",
        "      )(x)\n",
        "\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    if train_type.lower() == 'one-hot':\n",
        "      outputs = layers.Dense(vocab_size, activation='softmax', dtype='float32')(x)\n",
        "    else: ## Embeddings\n",
        "      outputs = layers.Dense(vocab_size, dtype='float32')(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3BO0erf7-ZW"
      },
      "outputs": [],
      "source": [
        "# Comparar arquitecturas\n",
        "print(\"Comparaci√≥n de arquitecturas en par√°metros entrenables:\")\n",
        "print(\"=\" * 50)\n",
        "for rnn_type in ['rnn', 'lstm', 'gru']:\n",
        "    model_temp = build_char_language_model(vocab_size, rnn_type=rnn_type)\n",
        "    print(f\"  {rnn_type.upper():>5}: {model_temp.count_params():>10,} par√°metros\")\n",
        "    del model_temp\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsCkQEm37-ZX"
      },
      "source": [
        "### Entrenamiento de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "BCRdwfCSXI48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso la clase PplCallback provista por la c√°tedra con early stopping basado en perplexity, padding de secuencias y paciencia de 3."
      ],
      "metadata": {
        "id": "pIIXMh2VUht7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JtBO7Qd-7-ZX"
      },
      "outputs": [],
      "source": [
        "class PplCallback(Callback):\n",
        "    '''\n",
        "    Este callback es una soluci√≥n ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la m√©trica de Perplejidad sobre un conjunto de datos de validaci√≥n.\n",
        "    La perplejidad es una m√©trica cuantitativa para evaluar la calidad de la generaci√≥n de secuencias.\n",
        "    Adem√°s implementa la finalizaci√≥n del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora despu√©s de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, max_context_size, history_ppl, patience=3):\n",
        "      # El callback lo inicializamos con secuencias de validaci√≥n sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "      self.max_context_size = max_context_size\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "      self.history_ppl = history_ppl\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validaci√≥n\n",
        "      for seq in self.val_data:\n",
        "          len_seq = len(seq)\n",
        "          # armamos todas las subsecuencias\n",
        "          subseq = [seq[:i] for i in range(1, len_seq)]\n",
        "          self.target.extend([seq[i] for i in range(1, len_seq)])\n",
        "\n",
        "          if len(subseq) != 0:\n",
        "              self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "              self.info.append((count, count + len_seq - 1))\n",
        "              count += len_seq - 1\n",
        "\n",
        "      if self.padded:\n",
        "          self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if len(self.padded) == 0:\n",
        "            print(\"\\nNo hay datos de validaci√≥n para calcular perplejidad\")\n",
        "            return\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        # Calcular predicciones\n",
        "        predictions = self.model.predict(self.padded, verbose=0)\n",
        "\n",
        "        # Calcular perplejidad para cada secuencia para cada secuencia de validaci√≥n\n",
        "        for start, end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los t√©rminos target\n",
        "          probs = [predictions[idx_seq, -1, idx_vocab]\n",
        "                     for idx_seq, idx_vocab in zip(range(start, end), self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos:  exp(-mean(log(probs)))\n",
        "          if len(probs) > 0:\n",
        "            scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores) if scores else np.inf\n",
        "        self.history_ppl.append(current_score)\n",
        "        train_ppl = np.exp(logs.get('loss', 0)) if logs else np.inf\n",
        "        print(f\"\\n Epoch {epoch+1} | Train PPL: {train_ppl:7.2f} | Val PPL: {current_score:7.2f}\", end='')\n",
        "\n",
        "        # Early stopping basado en perplejidad\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"best_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          print(f\" (paciencia: {self.patience_counter}/{self.patience})\")\n",
        "\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Early stopping por perplejidad...\")\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construyo cada modelo seg√∫n al arquitectura deseada, con RMSprop para optimizaci√≥n como es recomendado.\n",
        "Defin√≠ la loss SparseCategoricalCrossentropy con  from_logits=False, dado que al usar one-hot ya se aplica softmax en la √∫ltima capa de la red."
      ],
      "metadata": {
        "id": "IYzA8jJkVLJ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FFW6V3X_7-ZX"
      },
      "outputs": [],
      "source": [
        "def train_model_keras(rnn_type, vocab_size, train_dataset, val_sequences,\n",
        "                      max_context_size, hidden_size=256, num_layers=2,\n",
        "                      embedding_dim=128, dropout=0.5, embed_dropout=0.2,\n",
        "                      learning_rate=0.001, weight_decay=1e-5,\n",
        "                      label_smoothing=0.1, num_epochs=30, patience=3,\n",
        "                      train_type='one-hot'):\n",
        "    \"\"\"Entrena un modelo de lenguaje con early stopping.\"\"\"\n",
        "    print(f\"\\nIniciando entrenamiento - {rnn_type.upper()}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model = build_char_language_model(\n",
        "        vocab_size, hidden_size, num_layers, rnn_type,\n",
        "        dropout, embedding_dim, embed_dropout, train_type\n",
        "    )\n",
        "\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    if train_type.lower() == 'one-hot':\n",
        "      loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    else: ## Embeddings\n",
        "      loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True #, label_smoothing=label_smoothing\n",
        "                                                         )\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "    history_ppl = []\n",
        "    callbacks = [\n",
        "        PplCallback(\n",
        "            val_data=val_sequences,\n",
        "            max_context_size=max_context_size,\n",
        "            history_ppl=history_ppl,\n",
        "            patience=patience\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=num_epochs,\n",
        "                        callbacks=callbacks,\n",
        "                        verbose=1)\n",
        "\n",
        "    history.history['val_perplexity'] = history_ppl\n",
        "    best_ppl = min(history_ppl) if history_ppl else np.inf\n",
        "\n",
        "    if history_ppl:\n",
        "        model = keras.models.load_model(\"best_model.keras\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Mejor perplejidad de validaci√≥n: {best_ppl:.2f}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trains\n",
        "\n",
        "A continuaci√≥n, creo 3 arquitecturas vistas en clase: SimpleRNN (Celda de Elman), LSTM y GRU.\n",
        "\n",
        "1. SimpleRNN:\n",
        "   *   Ser√° el baseline para evaluar arquitecturas m√°s complejas.\n",
        "   *   Es la m√°s r√°pida en entrenar, ya que tiene menos par√°metros y es la m√°s simple.\n",
        "   * Tiene el problema de vanishing gradients en secuencias largas y dificultad para capturar dependencias a largo plazo.\n",
        "   * Resultado: PPL =\n",
        "\n",
        "2. LSTM:\n",
        "   * Las celdas de memoria permiten preservar informaci√≥n relevante.\n",
        "   * Tiene mejor control de la informaci√≥n.\n",
        "   * Es menos susceptible a vanishing gradients gracias a las conexiones residuales impl√≠citas.\n",
        "   * Resultado: PPL =\n",
        "\n",
        "\n",
        "3. GRU:\n",
        "   * Tiene menos par√°metros que LSTM con rendimiento comparable y es m√°s simple.\n",
        "   * Es menos proclive al overfitting por tener menos par√°metros y generaliza m√°s en datasets chicos.\n",
        "   * Resultado: PPL ="
      ],
      "metadata": {
        "id": "APnnUXw2XNYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y5erWlqN7-ZX"
      },
      "outputs": [],
      "source": [
        "# Hiperpar√°metros\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 2\n",
        "EMBEDDING_DIM = 128\n",
        "DROPOUT = 0.5\n",
        "EMBED_DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 30\n",
        "PATIENCE = 3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "LABEL_SMOOTHING = 0.1\n",
        "\n",
        "models = {}\n",
        "histories = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EEZ5Li07-ZX",
        "outputId": "e516ece1-7830-4444-fde1-d1d839467f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ENTRENANDO MODELO: SimpleRNN\n",
            "======================================================================\n",
            "\n",
            "Iniciando entrenamiento - RNN\n",
            "======================================================================\n",
            "Epoch 1/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2005 - loss: 2.8626\n",
            " Epoch 1 | Train PPL:   14.49 | Val PPL:    9.70Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 54ms/step - accuracy: 0.2006 - loss: 2.8624\n",
            "Epoch 2/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2518 - loss: 2.5464\n",
            " Epoch 2 | Train PPL:   12.49 | Val PPL:    9.55Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.2518 - loss: 2.5464\n",
            "Epoch 3/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2649 - loss: 2.4907\n",
            " Epoch 3 | Train PPL:   12.07 | Val PPL:    9.15Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 48ms/step - accuracy: 0.2649 - loss: 2.4907\n",
            "Epoch 4/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2624 - loss: 2.5079\n",
            " Epoch 4 | Train PPL:   12.27 | Val PPL:   11.39 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 48ms/step - accuracy: 0.2624 - loss: 2.5079\n",
            "Epoch 5/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2673 - loss: 2.4880\n",
            " Epoch 5 | Train PPL:   11.64 | Val PPL:    7.07Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 48ms/step - accuracy: 0.2673 - loss: 2.4880\n",
            "Epoch 6/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2888 - loss: 2.3968\n",
            " Epoch 6 | Train PPL:   10.84 | Val PPL:    7.06Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.2888 - loss: 2.3968\n",
            "Epoch 7/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2972 - loss: 2.3649\n",
            " Epoch 7 | Train PPL:   10.55 | Val PPL:    6.74Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 48ms/step - accuracy: 0.2972 - loss: 2.3649\n",
            "Epoch 8/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3022 - loss: 2.3466\n",
            " Epoch 8 | Train PPL:   10.39 | Val PPL:    6.65Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3022 - loss: 2.3466\n",
            "Epoch 9/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3048 - loss: 2.3371\n",
            " Epoch 9 | Train PPL:   10.30 | Val PPL:    6.74 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 49ms/step - accuracy: 0.3048 - loss: 2.3371\n",
            "Epoch 10/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3060 - loss: 2.3352\n",
            " Epoch 10 | Train PPL:   10.23 | Val PPL:    6.39Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 49ms/step - accuracy: 0.3060 - loss: 2.3352\n",
            "Epoch 11/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3108 - loss: 2.3155\n",
            " Epoch 11 | Train PPL:   10.08 | Val PPL:    6.31Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3108 - loss: 2.3155\n",
            "Epoch 12/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3130 - loss: 2.3067\n",
            " Epoch 12 | Train PPL:    9.98 | Val PPL:    6.44 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3130 - loss: 2.3067\n",
            "Epoch 13/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3137 - loss: 2.3047\n",
            " Epoch 13 | Train PPL:    9.97 | Val PPL:    6.13Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 48ms/step - accuracy: 0.3137 - loss: 2.3047\n",
            "Epoch 14/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3163 - loss: 2.2945\n",
            " Epoch 14 | Train PPL:    9.87 | Val PPL:    6.13 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3163 - loss: 2.2945\n",
            "Epoch 15/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3168 - loss: 2.2932\n",
            " Epoch 15 | Train PPL:    9.86 | Val PPL:    5.92Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3168 - loss: 2.2932\n",
            "Epoch 16/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3187 - loss: 2.2865\n",
            " Epoch 16 | Train PPL:    9.78 | Val PPL:    5.82Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3187 - loss: 2.2865\n",
            "Epoch 17/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3197 - loss: 2.2829\n",
            " Epoch 17 | Train PPL:    9.76 | Val PPL:    6.11 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3197 - loss: 2.2829\n",
            "Epoch 18/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3205 - loss: 2.2799\n",
            " Epoch 18 | Train PPL:    9.71 | Val PPL:    5.80Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3205 - loss: 2.2799\n",
            "Epoch 19/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3224 - loss: 2.2728\n",
            " Epoch 19 | Train PPL:    9.66 | Val PPL:    5.77Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3224 - loss: 2.2728\n",
            "Epoch 20/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3230 - loss: 2.2709\n",
            " Epoch 20 | Train PPL:    9.63 | Val PPL:    5.74Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 49ms/step - accuracy: 0.3230 - loss: 2.2709\n",
            "Epoch 21/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3239 - loss: 2.2669\n",
            " Epoch 21 | Train PPL:    9.60 | Val PPL:    5.63Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 49ms/step - accuracy: 0.3239 - loss: 2.2669\n",
            "Epoch 22/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3245 - loss: 2.2652\n",
            " Epoch 22 | Train PPL:    9.59 | Val PPL:    5.69 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 49ms/step - accuracy: 0.3245 - loss: 2.2652\n",
            "Epoch 23/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3251 - loss: 2.2630\n",
            " Epoch 23 | Train PPL:    9.57 | Val PPL:    5.69 (paciencia: 2/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3251 - loss: 2.2630\n",
            "Epoch 24/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3257 - loss: 2.2615\n",
            " Epoch 24 | Train PPL:    9.55 | Val PPL:    5.61Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 49ms/step - accuracy: 0.3257 - loss: 2.2615\n",
            "Epoch 25/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3256 - loss: 2.2614\n",
            " Epoch 25 | Train PPL:    9.56 | Val PPL:    5.62 (paciencia: 1/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 48ms/step - accuracy: 0.3256 - loss: 2.2614\n",
            "Epoch 26/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3266 - loss: 2.2576\n",
            " Epoch 26 | Train PPL:    9.52 | Val PPL:    5.71 (paciencia: 2/3)\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 48ms/step - accuracy: 0.3266 - loss: 2.2576\n",
            "Epoch 27/30\n",
            "\u001b[1m1825/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3262 - loss: 2.2591\n",
            " Epoch 27 | Train PPL:    9.53 | Val PPL:    5.63 (paciencia: 3/3)\n",
            "Early stopping por perplejidad...\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 48ms/step - accuracy: 0.3262 - loss: 2.2591\n",
            "======================================================================\n",
            "Mejor perplejidad de validaci√≥n: 5.61\n"
          ]
        }
      ],
      "source": [
        "# Entrenar SimpleRNN\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENTRENANDO MODELO: SimpleRNN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "models['rnn'], histories['rnn'] = train_model_keras(\n",
        "    rnn_type='rnn',\n",
        "    vocab_size=vocab_size,\n",
        "    train_dataset=train_dataset,\n",
        "    val_sequences=tokenized_sentences_val,\n",
        "    max_context_size=MAX_CONTEXT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    embed_dropout=EMBED_DROPOUT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    label_smoothing=LABEL_SMOOTHING,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    patience=PATIENCE,\n",
        "    train_type='one-hot' #, train_type='embeddings'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz56iqfR7-ZX",
        "outputId": "8863277b-83e8-4562-a48c-5f2b1a307b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ENTRENANDO MODELO: LSTM\n",
            "======================================================================\n",
            "\n",
            "Iniciando entrenamiento - LSTM\n",
            "======================================================================\n",
            "Epoch 1/30\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.2554 - loss: 2.5712\n",
            " Epoch 1 | Train PPL:   10.47 | Val PPL:    7.43Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 354ms/step - accuracy: 0.2554 - loss: 2.5710\n",
            "Epoch 2/30\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.3566 - loss: 2.1015\n",
            " Epoch 2 | Train PPL:    7.85 | Val PPL:    6.40Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 350ms/step - accuracy: 0.3566 - loss: 2.1014\n",
            "Epoch 3/30\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.3934 - loss: 1.9704\n",
            " Epoch 3 | Train PPL:    6.97 | Val PPL:    5.84Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 352ms/step - accuracy: 0.3934 - loss: 1.9704\n",
            "Epoch 4/30\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.4197 - loss: 1.8796\n",
            " Epoch 4 | Train PPL:    6.40 | Val PPL:    5.43Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 353ms/step - accuracy: 0.4197 - loss: 1.8795\n",
            "Epoch 5/30\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.4405 - loss: 1.8083\n",
            " Epoch 5 | Train PPL:    5.97 | Val PPL:    5.14Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m655s\u001b[0m 359ms/step - accuracy: 0.4405 - loss: 1.8083\n",
            "Epoch 6/30\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.4577 - loss: 1.7503\n",
            " Epoch 6 | Train PPL:    5.65 | Val PPL:    4.89Saved new model!\n",
            "\u001b[1m1826/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 358ms/step - accuracy: 0.4577 - loss: 1.7503\n",
            "Epoch 7/30\n",
            "\u001b[1m1564/1826\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m40s\u001b[0m 155ms/step - accuracy: 0.4712 - loss: 1.7053"
          ]
        }
      ],
      "source": [
        "# Entrenar LSTM\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENTRENANDO MODELO: LSTM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "models['lstm'], histories['lstm'] = train_model_keras(\n",
        "    rnn_type='lstm',\n",
        "    vocab_size=vocab_size,\n",
        "    train_dataset=train_dataset,\n",
        "    val_sequences=tokenized_sentences_val,\n",
        "    max_context_size=MAX_CONTEXT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    embed_dropout=EMBED_DROPOUT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    label_smoothing=LABEL_SMOOTHING,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    patience=PATIENCE,\n",
        "    train_type='one-hot' #, train_type='embeddings'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPuif32s7-ZX"
      },
      "outputs": [],
      "source": [
        "# Entrenar GRU\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENTRENANDO MODELO: GRU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "models['gru'], histories['gru'] = train_model_keras(\n",
        "    rnn_type='gru',\n",
        "    vocab_size=vocab_size,\n",
        "    train_dataset=train_dataset,\n",
        "    val_sequences=tokenized_sentences_val,\n",
        "    max_context_size=MAX_CONTEXT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    embed_dropout=EMBED_DROPOUT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    label_smoothing=LABEL_SMOOTHING,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    patience=PATIENCE,\n",
        "    train_type='one-hot' #, train_type='embeddings'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqhUVhjz7-ZX"
      },
      "outputs": [],
      "source": [
        "colors = {'rnn': '#e74c3c', 'lstm': '#3498db', 'gru': '#2ecc71'}\n",
        "labels_map = {'rnn': 'SimpleRNN', 'lstm': 'LSTM', 'gru': 'GRU'}\n",
        "\n",
        "if histories:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Subplot 0: Perplexity de validaci√≥n (solo cuando exista)\n",
        "    for model_type, history in histories.items():\n",
        "        val_ppl = history.history.get('val_perplexity', None)\n",
        "        if val_ppl is None:\n",
        "            val_loss = history.history.get('val_loss', None)\n",
        "            if val_loss is not None:\n",
        "                val_ppl = [float(np.exp(l)) for l in val_loss]\n",
        "        if val_ppl:\n",
        "            epochs_val = range(1, len(val_ppl) + 1)\n",
        "            axes[0].plot(epochs_val, val_ppl, color=colors.get(model_type, None),\n",
        "                         label=labels_map.get(model_type, model_type), linewidth=2,\n",
        "                         marker='o', markersize=4)\n",
        "    axes[0].set_xlabel('√âpoca')\n",
        "    axes[0].set_ylabel('Perplejidad')\n",
        "    axes[0].set_title('Perplejidad de Validaci√≥n')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Subplot 1: curvas de loss (train vs val)\n",
        "    for model_type, history in histories.items():\n",
        "        loss = history.history.get('loss', [])\n",
        "        if loss:\n",
        "            epochs_train = range(1, len(loss) + 1)\n",
        "            axes[1].plot(epochs_train, loss, color=colors.get(model_type, None),\n",
        "                         label=f\"{labels_map.get(model_type, model_type)} (train)\", linestyle='-')\n",
        "\n",
        "        val_loss = history.history.get('val_loss', None)\n",
        "        val_ppl = history.history.get('val_perplexity', None)\n",
        "        if val_ppl:\n",
        "            epochs_val = range(1, len(val_ppl) + 1)\n",
        "            axes[1].plot(epochs_val, val_ppl, color=colors.get(model_type, None),\n",
        "                         label=f\"{labels_map.get(model_type, model_type)} (val PPL)\", linestyle='--')\n",
        "        elif val_loss:\n",
        "            epochs_val = range(1, len(val_loss) + 1)\n",
        "            axes[1].plot(epochs_val, val_loss, color=colors.get(model_type, None),\n",
        "                         label=f\"{labels_map.get(model_type, model_type)} (val loss)\", linestyle='--')\n",
        "\n",
        "    axes[1].set_xlabel('√âpoca')\n",
        "    axes[1].set_ylabel('Loss / PPL')\n",
        "    axes[1].set_title('Curvas de Aprendizaje')\n",
        "\n",
        "    handles, labels = axes[1].get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    axes[1].legend(by_label.values(), by_label.keys())\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nResumen de Modelos:\")\n",
        "    print(\"=\" * 55)\n",
        "    for model_type, history in histories.items():\n",
        "        val_ppl = history.history.get('val_perplexity', None)\n",
        "        if val_ppl:\n",
        "            best_ppl = min(val_ppl)\n",
        "        else:\n",
        "            val_loss = history.history.get('val_loss', None)\n",
        "            if val_loss:\n",
        "                best_ppl = float(np.exp(min(val_loss)))\n",
        "            else:\n",
        "                best_ppl = np.inf\n",
        "        if np.isfinite(best_ppl):\n",
        "            print(f\"  {labels_map.get(model_type, model_type):<10}: PPL = {best_ppl:.2f}\")\n",
        "        else:\n",
        "            print(f\"  {labels_map.get(model_type, model_type):<10}: PPL = N/A (sin datos de validaci√≥n)\")\n",
        "    print(\"=\" * 55)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxZZQ-ii7-ZX"
      },
      "source": [
        "## 4. Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determ√≠stico y estoc√°stico. En este √∫ltimo caso observar el efecto de la temperatura en la generaci√≥n de secuencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSMbsDMz7-ZX"
      },
      "outputs": [],
      "source": [
        "# Seleccionar mejor modelo\n",
        "best_model_type = min(histories, key=lambda x: min(histories[x].history.get('val_perplexity', [np.inf])))\n",
        "\n",
        "model = models[best_model_type]\n",
        "best_ppl = min(histories[best_model_type].history.get('val_perplexity', [np.inf]))\n",
        "\n",
        "print(f\"Usando modelo: {best_model_type.upper()} (PPL: {best_ppl:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "pwM9vdcMmxPA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC8wKQ2d7-ZX"
      },
      "outputs": [],
      "source": [
        "def greedy_search(model, seed_text, max_length, num_chars):\n",
        "    \"\"\"Genera texto usando b√∫squeda greedy.\"\"\"\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_chars):\n",
        "        tokens = [char2idx.get(ch, 0) for ch in generated_text[-max_length:]]\n",
        "        if len(tokens) < max_length:\n",
        "            tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "        #x = np.array([tokens], dtype=np.int32)\n",
        "        x = pad_sequences([tokens], maxlen=max_length, padding='pre')\n",
        "        logits = model.predict(x, verbose=0)\n",
        "        next_char_idx = np.argmax(logits[0, -1, :])\n",
        "        generated_text += idx2char[next_char_idx]\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk3d3OW97-ZY"
      },
      "outputs": [],
      "source": [
        "def sample_with_temperature(model, seed_text, max_length, num_chars, temperature=1.0):\n",
        "    \"\"\"Genera texto usando muestreo con temperatura.\"\"\"\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_chars):\n",
        "        tokens = [char2idx.get(ch, 0) for ch in generated_text[-max_length:]]\n",
        "        if len(tokens) < max_length:\n",
        "            tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "        #x = np.array([tokens], dtype=np.int32)\n",
        "        x = pad_sequences([tokens], maxlen=max_length, padding='pre')\n",
        "\n",
        "        logits = model.predict(x, verbose=0)\n",
        "        logits_scaled = logits[0, -1, :] / temperature\n",
        "        probs = tf.nn.softmax(logits_scaled).numpy()\n",
        "        next_char_idx = np.random.choice(len(probs), p=probs)\n",
        "        generated_text += idx2char[next_char_idx]\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVoYAoL67-ZY"
      },
      "outputs": [],
      "source": [
        "def beam_search_deterministic(model, seed_text, max_length, num_chars, beam_width=5):\n",
        "    \"\"\"Genera texto usando beam search determin√≠stico.\"\"\"\n",
        "    seed_text = seed_text.lower()\n",
        "    beams = [(seed_text, 0.0)]\n",
        "    for _ in range(num_chars):\n",
        "        all_candidates = []\n",
        "        for text, score in beams:\n",
        "            tokens = [char2idx.get(ch, 0) for ch in text[-max_length:]]\n",
        "            if len(tokens) < max_length:\n",
        "                tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "            #x = np.array([tokens], dtype=np.int32)\n",
        "            x = pad_sequences([tokens], maxlen=max_length, padding='pre')\n",
        "\n",
        "            logits = model.predict(x, verbose=0)\n",
        "            log_probs = tf.nn.log_softmax(logits[0, -1, :]).numpy()\n",
        "            top_indices = np.argsort(log_probs)[-beam_width:]\n",
        "            for idx in top_indices:\n",
        "                all_candidates.append((text + idx2char[idx], score + log_probs[idx]))\n",
        "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = all_candidates[:beam_width]\n",
        "    final_sequences = [(text, score / len(text)) for text, score in beams]\n",
        "    final_sequences.sort(key=lambda x: x[1], reverse=True)\n",
        "    return final_sequences[0][0], final_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6grF99897-ZY"
      },
      "outputs": [],
      "source": [
        "def beam_search_stochastic(model, seed_text, max_length, num_chars, beam_width=5, temperature=1.0):\n",
        "    \"\"\"Genera texto usando beam search estoc√°stico.\"\"\"\n",
        "    seed_text = seed_text.lower()\n",
        "    beams = [(seed_text, 0.0)]\n",
        "    for _ in range(num_chars):\n",
        "        all_candidates = []\n",
        "        for text, score in beams:\n",
        "            tokens = [char2idx.get(ch, 0) for ch in text[-max_length:]]\n",
        "            if len(tokens) < max_length:\n",
        "                tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "            #x = np.array([tokens], dtype=np.int32)\n",
        "            x = pad_sequences([tokens], maxlen=max_length, padding='pre')\n",
        "\n",
        "            logits = model.predict(x, verbose=0)\n",
        "            logits_scaled = logits[0, -1, :] / temperature\n",
        "            probs = tf.nn.softmax(logits_scaled).numpy()\n",
        "            log_probs = np.log(probs + 1e-10)\n",
        "            sampled_indices = np.random.choice(len(probs), size=min(beam_width, len(probs)),\n",
        "                                               replace=False, p=probs)\n",
        "            for idx in sampled_indices:\n",
        "                all_candidates.append((text + idx2char[idx], score + log_probs[idx]))\n",
        "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = all_candidates[:beam_width]\n",
        "    final_sequences = [(text, score / len(text)) for text, score in beams]\n",
        "    final_sequences.sort(key=lambda x: x[1], reverse=True)\n",
        "    return final_sequences[0][0], final_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lisis\n",
        "\n",
        "1. GREEDY\n",
        "    * Es determin√≠stico. Siempre produce el mismo resultado para la misma semilla.\n",
        "    * Es muy r√°pido porque solo elige el m√°s probable en cada paso.\n",
        "    * Tiene tendencia a responder loops.\n",
        "\n",
        "2. BEAM SEARCH\n",
        "    * Selecciona los mejores n beams y avanza al siguiente nivel repitiendo el proceso.\n",
        "    * Selecciona la secuencia con mejor score normalizado.\n",
        "    * Sigue siendo determin√≠stico, aunque mejor que GREEDY.\n",
        "3. BEAM STOCHASTIC\n",
        "    * Muestrea k opciones seg√∫n una probabilidad dada.\n",
        "    * Genera textos diferentes en cada ejecuci√≥n.\n",
        "    * La temperatura permite ajustar creatividad."
      ],
      "metadata": {
        "id": "RSQl514kmsvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umDkgTno7-ZY"
      },
      "outputs": [],
      "source": [
        "# Ejemplos de generaci√≥n\n",
        "seed = \"ulises dijo\"\n",
        "print(\"=\"*70)\n",
        "print(f\"COMPARACI√ìN DE M√âTODOS - Semilla: '{seed}'\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nGREEDY:\")\n",
        "print(greedy_search(model, seed, MAX_CONTEXT_SIZE, 100))\n",
        "\n",
        "#print(\"\\nSAMPLING (T=0.7):\")\n",
        "#print(sample_with_temperature(model, seed, MAX_CONTEXT_SIZE, 100, 0.7))\n",
        "\n",
        "print(\"\\nBEAM SEARCH (width=5):\")\n",
        "result, _ = beam_search_deterministic(model, seed, MAX_CONTEXT_SIZE, 100, 5)\n",
        "print(result)\n",
        "\n",
        "print(\"\\nBEAM STOCHASTIC (width=5, T=0.7):\")\n",
        "result, _ = beam_search_stochastic(model, seed, MAX_CONTEXT_SIZE, 100, 5, 0.7)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La temperatura es el hiperpar√°metro m√°s importante para controlar el estilo de generaci√≥n: valores bajos para m√°s determinismo, valores altos para exploraci√≥n creativa.\n",
        "\n",
        "T\t| Comportamiento esperado\n",
        "--|--\n",
        "0.2\t| Texto muy repetitivo pero gramaticalmente correcto\n",
        "0.5\t| Buen equilibrio, texto fluido\n",
        "0.8\t| M√°s creativo, ocasionales sorpresas\n",
        "1.0\t| Distribuci√≥n original del modelo\n",
        "1.5\t| Alta creatividad, posibles errores"
      ],
      "metadata": {
        "id": "gKEm24Dtpt_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du9MNxzE7-ZY"
      },
      "outputs": [],
      "source": [
        "# Efecto de la temperatura\n",
        "seed = \"el h√©roe regres√≥\"\n",
        "print(\"=\"*70)\n",
        "print(f\"EFECTO DE LA TEMPERATURA - Semilla: '{seed}'\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for temp in [0.2, 0.5, 0.8, 1.0, 1.5]:\n",
        "    print(f\"\\nT = {temp}:\")\n",
        "    print(beam_search_stochastic(model, seed, MAX_CONTEXT_SIZE, 80, 5, temp))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIM9SYUK7-ZY"
      },
      "outputs": [],
      "source": [
        "# Guardar modelo\n",
        "model.save('best_char_lm_keras.keras')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}