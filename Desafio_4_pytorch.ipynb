{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesphamm/procesamiento_lenguaje_natural/blob/main/Desafio_4_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agWuQGwRfhat"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## LSTM Traductor (PyTorch)\n",
        "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzG2qRWyfhau"
      },
      "source": [
        "### 1 - Datos\n",
        "El objecto es utilizar datos disponibles del Tatoeba Project de traducciones de texto en diferentes idiomas. Se construirá un modelo traductor de inglés a español seq2seq utilizando encoder-decoder.\\\n",
        "[LINK](https://www.manythings.org/anki/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8W_xR8zEfhau"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk5SVAkkfhau",
        "outputId": "0773f217-b33c-4879-c5e0-410b3da35c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA disponible: False\n",
            "Dispositivo: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Dispositivo: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8K6kh8fCfhav"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/cespinola/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GAvK3Iofhav",
        "outputId": "21a43b02-d3ae-4c48-cad1-f00d24f1855f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El dataset ya se encuentra descargado\n"
          ]
        }
      ],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "\n",
        "import os\n",
        "if os.access('spa-eng', os.F_OK) is False:\n",
        "    if os.access('spa-eng.zip', os.F_OK) is False:\n",
        "        !curl -L -o 'spa-eng.zip' 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
        "    !unzip -q spa-eng.zip\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IVuFVyd6fhav"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True  # Optimiza kernels para tu hardware\n",
        "    torch.backends.cudnn.deterministic = False  # Permite optimizaciones no deterministas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYQfo2ddfhav",
        "outputId": "6dfdace7-f55e-48e7-9447-1fe68802b843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows disponibles: 118964\n",
            "Cantidad de rows utilizadas: 118964\n"
          ]
        }
      ],
      "source": [
        "# dataset_file\n",
        "\n",
        "text_file = \"./spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "# Ahora podemos usar más datos gracias al DataLoader\n",
        "# que carga los batches on-the-fly sin consumir toda la RAM\n",
        "MAX_NUM_SENTENCES = 118964 #50000\n",
        "\n",
        "# Mezclar el dataset, forzar semilla siempre igual\n",
        "np.random.seed([40])\n",
        "np.random.shuffle(lines)\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "count = 0\n",
        "\n",
        "for line in lines:\n",
        "    count += 1\n",
        "    if count > MAX_NUM_SENTENCES:\n",
        "        break\n",
        "\n",
        "    # el tabulador señaliza la separación entre las oraciones\n",
        "    # en ambos idiomas\n",
        "    if '\\t' not in line:\n",
        "        continue\n",
        "\n",
        "    # Input sentence --> eng\n",
        "    # output --> spa\n",
        "    input_sentence, output = line.rstrip().split('\\t')\n",
        "\n",
        "    # output sentence (decoder_output) tiene <eos>\n",
        "    output_sentence = output + ' <eos>'\n",
        "    # output sentence input (decoder_input) tiene <sos>\n",
        "    output_sentence_input = '<sos> ' + output\n",
        "\n",
        "    input_sentences.append(input_sentence)\n",
        "    output_sentences.append(output_sentence)\n",
        "    output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows disponibles:\", len(lines))\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX-tPR7Mfhaw",
        "outputId": "c79516bf-8c4d-48fd-9dea-d3cbccccfffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('A deal is a deal.',\n",
              " 'Un trato es un trato. <eos>',\n",
              " '<sos> Un trato es un trato.')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentences[0], output_sentences[0], output_sentences_inputs[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAN3ucoQfhaw"
      },
      "source": [
        "### 2 - Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AcDe_y9Vfhaw"
      },
      "outputs": [],
      "source": [
        "# Definir el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "# Vamos a necesitar un tokenizador para cada idioma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnmkwNyDfhaw",
        "outputId": "5165cb32-4d57-403a-a99a-e34e0b14faf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 13524\n",
            "Sentencia de entrada más larga: 47\n"
          ]
        }
      ],
      "source": [
        "# Tokenizar las palabras con el Tokenizer de Keras\n",
        "# Definir una máxima cantidad de palabras a utilizar:\n",
        "# - num_words --> el número máximo de palabras a conservar, en función de la frecuencia de las palabras.\n",
        "# - Solo se conservarán las num_words-1 palabras más comunes.\n",
        "\n",
        "# tokenizador de inglés\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Sentencia de entrada más larga:\", max_input_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSBetV0zfhaw",
        "outputId": "5c7d5b3c-d99b-4ff6-8f9e-539cc98ca32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 26341\n",
            "Sentencia de salida más larga: 50\n"
          ]
        }
      ],
      "source": [
        "# tokenizador de español\n",
        "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
        "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
        "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE)\n",
        "# Se suma 1 para incluir el token de palabra desconocida\n",
        "\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Sentencia de salida más larga:\", max_out_len)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPMwwoBbfhaw"
      },
      "source": [
        "Como era de esperarse, las sentencias en castellano son más largas que en inglés, y lo mismo sucede con su vocabulario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pqe5zmnhfhaw"
      },
      "outputs": [],
      "source": [
        "# Por una cuestion de que no explote la RAM se limitará el tamaño de las sentencias de entrada\n",
        "# a la mitad:\n",
        "max_input_len = 16\n",
        "max_out_len = 18\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6SxRLdmfhaw"
      },
      "source": [
        "A la hora de realiza padding es importante tener en cuenta que en el encoder los ceros se agregan al comienzo y en el decoder al final. Esto es porque la salida del encoder está basado en las últimas palabras de la sentencia (son las más importantes), mientras que en el decoder está basado en el comienzo de la secuencia de salida ya que es la realimentación del sistema y termina con fin de sentencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-tJpL8Tfhax",
        "outputId": "7d2161d1-598b-40ff-b0fb-f099a87b7fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows del dataset: 118964\n",
            "encoder_input_sequences shape: (118964, 16)\n",
            "decoder_input_sequences shape: (118964, 18)\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
        "\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtRpUsdMfhax"
      },
      "source": [
        "La última capa del modelo (softmax) necesita que los valores de salida\n",
        "del decoder (decoder_sequences) estén en formato oneHotEncoder.\\\n",
        "Se utiliza \"decoder_output_sequences\" con la misma estrategia con que se transformó la entrada del decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt9wOFzdfhax",
        "outputId": "f54bcef1-c291-4d5a-eb71-fe03dd8d6876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoder_output_sequences shape: (118964, 18)\n"
          ]
        }
      ],
      "source": [
        "# Ya no creamos decoder_targets completo en memoria\n",
        "# Solo guardamos las secuencias de índices, PyTorch CrossEntropyLoss trabaja con índices\n",
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38hX7GAbfhax"
      },
      "source": [
        "#### Dataset para optimizar el uso de RAM\n",
        "\n",
        "El problema principal de memoria es que `decoder_targets` (el one-hot encoding de las salidas) consume muchísima RAM:\n",
        "- Con 50,000 muestras × 18 timesteps × 6,000 palabras vocabulario × 4 bytes = **~21 GB de RAM**\n",
        "\n",
        "El `Dataset` de PyTorch:\n",
        "1. Hereda de `torch.utils.data.Dataset` para integrarse con PyTorch\n",
        "2. PyTorch CrossEntropyLoss trabaja con índices directamente, sin necesidad de one-hot\n",
        "3. El DataLoader genera los batches on-the-fly\n",
        "4. Permite mezclar los datos (shuffle) usando DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZBgHxCAnfhax"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset para el modelo de traducción seq2seq en PyTorch.\n",
        "    Genera muestras on-the-fly para evitar cargar todo el dataset en RAM.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder_input: Secuencias de entrada del encoder (ya con padding)\n",
        "            decoder_input: Secuencias de entrada del decoder (ya con padding)\n",
        "            decoder_output: Secuencias de salida del decoder (índices, NO one-hot)\n",
        "        \"\"\"\n",
        "        self.encoder_input = torch.LongTensor(encoder_input)\n",
        "        self.decoder_input = torch.LongTensor(decoder_input)\n",
        "        self.decoder_output = torch.LongTensor(decoder_output)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoder_input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.encoder_input[idx],\n",
        "                self.decoder_input[idx],\n",
        "                self.decoder_output[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv57IPJQfhax"
      },
      "source": [
        "### 3 - Preparar los embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bPaBbu-Yfhax"
      },
      "outputs": [],
      "source": [
        "#import sys\n",
        "#!{sys.executable} -m pip install gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMmdyr6bfhax",
        "outputId": "82485d4c-952c-40bb-c393-93b22bc62b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los embeddings gloveembedding.pkl ya están descargados\n"
          ]
        }
      ],
      "source": [
        "# Descargar los embeddings desde un google drive (es la forma más rápida)\n",
        "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
        "# disponibles descargar de la página oficial como se explica en el siguiente bloque de código\n",
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XURvEOMxfhax"
      },
      "outputs": [],
      "source": [
        "# En caso de que gdown de algún error de permisos intentar descargar los\n",
        "# embeddings con curl:\n",
        "\n",
        "#!curl -L -o 'gloveembedding.pkl' 'https://drive.google.com/u/0/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download&confirm=t'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gNOgbx4mfhax"
      },
      "outputs": [],
      "source": [
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DQ0osHLmfhay"
      },
      "outputs": [],
      "source": [
        "# Por una cuestion de RAM se utilizarán los embeddings de Glove de dimension 50\n",
        "model_embeddings = GloveEmbeddings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RudN1oiHfhay",
        "outputId": "0ec7df74-7cb7-47ae-949c-1300804a5b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 219\n"
          ]
        }
      ],
      "source": [
        "# Crear la Embedding matrix de las secuencias\n",
        "# en inglés\n",
        "\n",
        "print('preparing embedding matrix...')\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "# word_index provieen del tokenizer\n",
        "\n",
        "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "\n",
        "if embedding_matrix.shape[0] < (np.max(encoder_input_sequences) + 1):\n",
        "    diff = (np.max(encoder_input_sequences) + 1) - embedding_matrix.shape[0]\n",
        "    embedding_matrix = np.vstack([embedding_matrix,\n",
        "                                  np.zeros((diff, embedding_matrix.shape[1]))])\n",
        "\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahVv8LSIfhay",
        "outputId": "4fdea652-d61c-4258-e233-8adff5e8ab6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13525, 50)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dimensión de los embeddings de la secuencia en inglés\n",
        "embedding_matrix.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGIjgKBnfhay"
      },
      "source": [
        "### 4 - Entrenar el modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR5719Xgfhay"
      },
      "source": [
        "#### Construir modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2WIQ415fhay",
        "outputId": "f8d3df16-7b0a-4ba9-aad1-f42a682c6e07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_dim = model_embeddings.N_FEATURES\n",
        "max_input_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nav0HzA8fhay"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_matrix, n_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        #vocab_size, embed_dim = embedding_matrix.shape\n",
        "        #self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        #self.embedding.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n",
        "        #self.embedding.weight.requires_grad = False  # No entrenable\n",
        "        #self.lstm = nn.LSTM(embed_dim, n_units, batch_first=True)\n",
        "\n",
        "        self.lstm_size = n_units\n",
        "        self.num_layers = 1\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers) # LSTM layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out)\n",
        "        return (ht, ct)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, num_words_output, n_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
        "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
        "        self.lstm_size = n_units\n",
        "        self.num_layers = 1\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.output_dim = num_words_output\n",
        "\n",
        "        #self.embedding = nn.Embedding(num_words_output, n_units)\n",
        "        #self.lstm = nn.LSTM(n_units, n_units, batch_first=True)\n",
        "        #self.fc = nn.Linear(n_units, num_words_output)\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers) # LSTM layer\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n",
        "\n",
        "    #def forward(self, x, h, c):\n",
        "    #    x = self.embedding(x)\n",
        "    #    outputs, (h, c) = self.lstm(x, (h, c))\n",
        "    #    predictions = self.fc(outputs)\n",
        "    #    return predictions, h, c\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
        "        out = self.softmax(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n",
        "        return out, (ht, ct)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        assert encoder.lstm_size == decoder.lstm_size, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        batch_size = decoder_input.shape[0]\n",
        "        decoder_input_len = decoder_input.shape[1]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor para almacenar la salida\n",
        "        # (batch_size, sentence_len, one_hot_size)\n",
        "        outputs = torch.zeros(\n",
        "            batch_size, decoder_input_len, vocab_size, device=decoder_input.device)\n",
        "\n",
        "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
        "        prev_state = self.encoder(encoder_input)\n",
        "\n",
        "        # En la primera iteracion se toma el primer token de target ()\n",
        "        input = decoder_input[:, 0:1]\n",
        "\n",
        "        for t in range(decoder_input_len):\n",
        "            # t --> token index\n",
        "\n",
        "            # utilizamos método \"teacher forcing\", es decir que durante\n",
        "            # el entrenamiento no realimentamos la salida del decoder\n",
        "            # sino el token correcto que sigue en target\n",
        "            input = decoder_input[:, t:t+1]\n",
        "\n",
        "            # ingresar cada token embedding, uno por uno junto al hidden state\n",
        "            # recibir el output del decoder (softmax)\n",
        "            output, prev_state = self.decoder(input, prev_state)\n",
        "            top1 = output.argmax(1).view(-1, 1)\n",
        "\n",
        "            # Sino se usará \"teacher forcing\" habría que descomentar\n",
        "            # esta linea.\n",
        "            # Hay ejemplos dandos vuelta en donde se utilza un random\n",
        "            # para ver en cada vuelta que técnica se aplica\n",
        "            #input = top1\n",
        "\n",
        "            # guardar cada salida (softmax)\n",
        "            outputs[:, t, :] = output\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OF7RAd-Sfhay"
      },
      "outputs": [],
      "source": [
        "def build_traductor_model(n_units=128, encoder_input_sequences_m=None, embedding_matrix_m=None):\n",
        "    \"\"\"Construye un modelo de traducción seq2seq.\"\"\"\n",
        "\n",
        "    encoder_input_sequences_m = np.clip(\n",
        "        encoder_input_sequences_m,\n",
        "        0,\n",
        "        embedding_matrix_m.shape[0] - 1\n",
        "    )\n",
        "\n",
        "    vocab_size = embedding_matrix_m.shape[0]\n",
        "\n",
        "    encoder = Encoder(vocab_size=vocab_size, embedding_matrix=embedding_matrix_m, n_units=n_units)\n",
        "    decoder = Decoder(vocab_size=vocab_size, num_words_output=num_words_output, n_units=n_units)\n",
        "    model = Seq2Seq(encoder=encoder, decoder=decoder)\n",
        "    model = model.to(device)\n",
        "    if hasattr(torch, 'compile'):\n",
        "        model = torch.compile(model, mode='reduce-overhead')\n",
        "\n",
        "    model_env = {\n",
        "        'model': model,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'encoder_input_sequences': encoder_input_sequences_m,\n",
        "        'n_units': n_units\n",
        "    }\n",
        "    return model_env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "70Qj6p4wfhaz",
        "outputId": "12d24e51-195d-4360-b89a-1733a0d0d837"
      },
      "outputs": [],
      "source": [
        "n_units_list = [64, 128, 256]\n",
        "#n_units_list = [128, 256]\n",
        "models = {f'n_units_{n_units}': None for n_units in n_units_list}\n",
        "\n",
        "for n_units in n_units_list:\n",
        "    model_env = build_traductor_model(n_units=n_units, encoder_input_sequences_m=encoder_input_sequences, embedding_matrix_m=embedding_matrix)\n",
        "    models[f'n_units_{n_units}'] = model_env\n",
        "    del model_env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrijJMiDfhaz"
      },
      "source": [
        "#### Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zw142hakfhaz"
      },
      "outputs": [],
      "source": [
        "m_64 = models['n_units_64']\n",
        "m_128 = models['n_units_128']\n",
        "m_256 = models['n_units_256']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLmFM1UOfhaz",
        "outputId": "b17a3561-4cf5-456d-f33e-ff7ad0bce7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen del modelo con 64 unidades LSTM:\n",
            "OptimizedModule(\n",
            "  (_orig_mod): Seq2Seq(\n",
            "    (encoder): Encoder(\n",
            "      (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "      (lstm): LSTM(50, 64, batch_first=True)\n",
            "    )\n",
            "    (decoder): Decoder(\n",
            "      (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "      (lstm): LSTM(50, 64, batch_first=True)\n",
            "      (fc1): Linear(in_features=64, out_features=25000, bias=True)\n",
            "      (softmax): Softmax(dim=1)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Resumen del modelo con 64 unidades LSTM:\")\n",
        "print(m_64['model'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oAKtLWifha0",
        "outputId": "e620d319-b96e-4428-bc13-b3a8a1abb18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo con 128 unidades:\n",
            "OptimizedModule(\n",
            "  (_orig_mod): Seq2Seq(\n",
            "    (encoder): Encoder(\n",
            "      (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "      (lstm): LSTM(50, 128, batch_first=True)\n",
            "    )\n",
            "    (decoder): Decoder(\n",
            "      (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "      (lstm): LSTM(50, 128, batch_first=True)\n",
            "      (fc1): Linear(in_features=128, out_features=25000, bias=True)\n",
            "      (softmax): Softmax(dim=1)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(f\"Modelo con 128 unidades:\")\n",
        "print(m_128['model'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZujZsc69fha8",
        "outputId": "a3c160a1-c8c4-43c1-877a-cb71b0c7a269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo con 256 unidades:\n",
            "OptimizedModule(\n",
            "  (_orig_mod): Seq2Seq(\n",
            "    (encoder): Encoder(\n",
            "      (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "      (lstm): LSTM(50, 256, batch_first=True)\n",
            "    )\n",
            "    (decoder): Decoder(\n",
            "      (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "      (lstm): LSTM(50, 256, batch_first=True)\n",
            "      (fc1): Linear(in_features=256, out_features=25000, bias=True)\n",
            "      (softmax): Softmax(dim=1)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(f\"Modelo con 256 unidades:\")\n",
        "print(m_256['model'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crb6RS6Lfha8"
      },
      "source": [
        "#### Visualizar arquitectura\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEl8tsl9fha8",
        "outputId": "da9c471f-600e-448d-c59a-9bbf1da5f935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquitectura del encoder 64:\n",
            "Encoder(\n",
            "  (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "  (lstm): LSTM(50, 64, batch_first=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Arquitectura del encoder 64:\")\n",
        "print(m_64['encoder'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOPYJNtTfha8",
        "outputId": "0d78ee1f-e9b3-40c2-cd16-31349f9187e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquitectura del decoder 64:\n",
            "Decoder(\n",
            "  (embedding): Embedding(13525, 50, padding_idx=0)\n",
            "  (lstm): LSTM(50, 64, batch_first=True)\n",
            "  (fc1): Linear(in_features=64, out_features=25000, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Arquitectura del decoder 64:\")\n",
        "print(m_64['decoder'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1MAmLIefha8"
      },
      "source": [
        "#### Entrenar modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lif7gt2Ofha8"
      },
      "outputs": [],
      "source": [
        "def train_model(model_env, train_loader, val_loader, epochs=50, patience=3):\n",
        "    \"\"\"Entrena el modelo con early stopping.\n",
        "    Añade mediciones de tiempo por época y por step (train y val).\n",
        "    \"\"\"\n",
        "    model = model_env['model']\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignorar padding\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Scheduler para ajustar learning rate\n",
        "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience)\n",
        "\n",
        "    history = {\n",
        "        'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': [],\n",
        "        'epoch_time': [], 'avg_train_step_time': [], 'avg_val_step_time': []\n",
        "    }\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    print(f\"Entrenando modelo con {epochs} épocas y paciencia {patience}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        train_step_time_total = 0.0\n",
        "        train_step_count = 0\n",
        "\n",
        "        epoch_start = time.perf_counter()\n",
        "\n",
        "        for encoder_input, decoder_input, decoder_output in train_loader:\n",
        "            step_start = time.perf_counter()\n",
        "\n",
        "            encoder_input = encoder_input.to(device, non_blocking=True)\n",
        "            decoder_input = decoder_input.to(device, non_blocking=True)\n",
        "            decoder_output = decoder_output.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(encoder_input, decoder_input)\n",
        "\n",
        "            # Reshape para calcular loss\n",
        "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
        "            targets_flat = decoder_output.view(-1)\n",
        "\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping para estabilidad\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "            # Si usamos GPU, sincronizar antes de medir tiempo del step\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "            step_end = time.perf_counter()\n",
        "\n",
        "            train_step_time_total += (step_end - step_start)\n",
        "            train_step_count += 1\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calcular accuracy\n",
        "            with torch.no_grad():\n",
        "                _, predicted = outputs.max(dim=-1)\n",
        "                mask = decoder_output != 0\n",
        "                train_correct += (predicted[mask] == decoder_output[mask]).sum().item()\n",
        "                train_total += mask.sum().item()\n",
        "\n",
        "        epoch_end_after_train = time.perf_counter()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = train_correct / train_total if train_total > 0 else 0\n",
        "        avg_train_step_time = (train_step_time_total / train_step_count) if train_step_count > 0 else 0.0\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        val_step_time_total = 0.0\n",
        "        val_step_count = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for encoder_input, decoder_input, decoder_output in val_loader:\n",
        "                val_step_start = time.perf_counter()\n",
        "\n",
        "                encoder_input = encoder_input.to(device, non_blocking=True)\n",
        "                decoder_input = decoder_input.to(device, non_blocking=True)\n",
        "                decoder_output = decoder_output.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(encoder_input, decoder_input)\n",
        "\n",
        "                outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
        "                targets_flat = decoder_output.view(-1)\n",
        "\n",
        "                loss = criterion(outputs_flat, targets_flat)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(dim=-1)\n",
        "                mask = decoder_output != 0\n",
        "                val_correct += (predicted[mask] == decoder_output[mask]).sum().item()\n",
        "                val_total += mask.sum().item()\n",
        "\n",
        "                # Sincronizar GPU si aplica antes de medir tiempo del step\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.synchronize()\n",
        "                val_step_end = time.perf_counter()\n",
        "\n",
        "                val_step_time_total += (val_step_end - val_step_start)\n",
        "                val_step_count += 1\n",
        "\n",
        "        epoch_end = time.perf_counter()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
        "        avg_val_step_time = (val_step_time_total / val_step_count) if val_step_count > 0 else 0.0\n",
        "\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        # Tiempo sólo de training dentro de la época\n",
        "        train_only_time = epoch_end_after_train - epoch_start\n",
        "\n",
        "        history['loss'].append(train_loss)\n",
        "        history['accuracy'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_acc)\n",
        "        history['epoch_time'].append(epoch_time)\n",
        "        history['avg_train_step_time'].append(avg_train_step_time)\n",
        "        history['avg_val_step_time'].append(avg_val_step_time)\n",
        "\n",
        "        print(\n",
        "            f'Epoch {epoch+1}/{epochs} - loss: {train_loss:.4f} - accuracy: {train_acc:.4f} - '\n",
        "            f'val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f}\\n'\n",
        "            f'    epoch_time: {epoch_time:.3f}s (train_time: {train_only_time:.3f}s) - '\n",
        "            f'avg_train_step: {avg_train_step_time*1000:.3f} ms - '\n",
        "            f'avg_val_step: {avg_val_step_time*1000:.3f} ms'\n",
        "        )\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping en la época {epoch+1}')\n",
        "                model.load_state_dict(best_state)\n",
        "                break\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sb0Wbp7fha9",
        "outputId": "b03d9dac-11f6-4b3d-a02c-22caee26fb21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos de entrenamiento: 95171 muestras\n",
            "Datos de validación: 23793 muestras\n"
          ]
        }
      ],
      "source": [
        "# Dividimos los datos en train/validation\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split de datos (80% train, 20% validation)\n",
        "indices = np.arange(len(encoder_input_sequences))\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Datos de entrenamiento\n",
        "encoder_input_train = encoder_input_sequences[train_idx]\n",
        "decoder_input_train = decoder_input_sequences[train_idx]\n",
        "decoder_output_train = decoder_output_sequences[train_idx]\n",
        "\n",
        "# Datos de validación\n",
        "encoder_input_val = encoder_input_sequences[val_idx]\n",
        "decoder_input_val = decoder_input_sequences[val_idx]\n",
        "decoder_output_val = decoder_output_sequences[val_idx]\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(train_idx)} muestras\")\n",
        "print(f\"Datos de validación: {len(val_idx)} muestras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2CJVurTfha9",
        "outputId": "adc835c5-c0bb-47e1-b938-ea8da21b9eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando modelo con 50 épocas y paciencia 10\n",
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = TranslationDataset(\n",
        "    encoder_input=encoder_input_train,\n",
        "    decoder_input=decoder_input_train,\n",
        "    decoder_output=decoder_output_train\n",
        ")\n",
        "\n",
        "val_dataset = TranslationDataset(\n",
        "    encoder_input=encoder_input_val,\n",
        "    decoder_input=decoder_input_val,\n",
        "    decoder_output=decoder_output_val\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,)\n",
        "                          #num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,)\n",
        "                        #num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "hist = train_model(m_64, train_loader, val_loader, epochs=50, patience=10)\n",
        "models['n_units_64']['hist'] = hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqzN_wf3fha9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2\n",
        ")\n",
        "\n",
        "hist = train_model(m_128, train_loader, val_loader, epochs=50, patience=10)\n",
        "models['n_units_128']['hist'] = hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TXA_nv3fha9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "hist = train_model(m_256, train_loader, val_loader, epochs=50, patience=3)\n",
        "models['n_units_256']['hist'] = hist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOnknNjvfha9"
      },
      "source": [
        "#### Comparar modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOionPmifha9"
      },
      "outputs": [],
      "source": [
        "for n_units in n_units_list:\n",
        "    hist = models[f'n_units_{n_units}']['hist']\n",
        "    print(f\"Resultados del modelo con {n_units} unidades:\")\n",
        "    # Entrenamiento\n",
        "    epoch_count = range(1, len(hist['accuracy']) + 1)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.lineplot(x=epoch_count,  y=hist['loss'], label='train')\n",
        "    sns.lineplot(x=epoch_count,  y=hist['val_loss'], label='valid')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.lineplot(x=epoch_count,  y=hist['accuracy'], label='train')\n",
        "    sns.lineplot(x=epoch_count,  y=hist['val_accuracy'], label='valid')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdQgE3aZfha9"
      },
      "source": [
        "Se selecciona el modelo con mejores métricas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u0hkOqufha9"
      },
      "outputs": [],
      "source": [
        "for name, model_data in models.items():\n",
        "    history = model_data['hist']\n",
        "    if history and 'accuracy' in history:\n",
        "        current_val_loss = history['accuracy'][-1]\n",
        "        print(f\"Model {name}: Final Accuracy = {current_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Unr0ZluKfha9"
      },
      "outputs": [],
      "source": [
        "best_model = models['n_units_128']\n",
        "best_encoder = best_model['encoder']\n",
        "best_decoder = best_model['decoder']\n",
        "best_n_units = best_model['n_units']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeyrTBQfha9"
      },
      "source": [
        "### 5 - Inferencia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3Tl5GSfha9"
      },
      "source": [
        "``` python\n",
        "Step 1:\n",
        "A deal is a deal -> Encoder -> enc(h1,c1)\n",
        "\n",
        "enc(h1,c1) + <sos> -> Decoder -> Un + dec(h1,c1)\n",
        "\n",
        "step 2:\n",
        "dec(h1,c1) + Un -> Decoder -> trato + dec(h2,c2)\n",
        "\n",
        "step 3:\n",
        "dec(h2,c2) + trato -> Decoder -> es + dec(h3,c3)\n",
        "\n",
        "step 4:\n",
        "dec(h3,c3) + es -> Decoder -> un + dec(h4,c4)\n",
        "\n",
        "step 5:\n",
        "dec(h4,c4) + un -> Decoder -> trato + dec(h5,c5)\n",
        "\n",
        "step 6:\n",
        "dec(h5,c5) + trato. -> Decoder -> <eos> + dec(h6,c6)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxj03eJdfha9"
      },
      "outputs": [],
      "source": [
        "# Armar los conversores de índice a palabra:\n",
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4AP_1Fgfha9"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(input_seq):\n",
        "    best_encoder.eval()\n",
        "    best_decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Convertir a tensor y mover al dispositivo\n",
        "        input_tensor = torch.LongTensor(input_seq).to(device)\n",
        "\n",
        "        # Se transforma la secuencia de entrada a los estados \"h\" y \"c\" de la LSTM\n",
        "        h, c = best_encoder(input_tensor)\n",
        "\n",
        "        # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "        target_seq = torch.LongTensor([[word2idx_outputs['<sos>']]]).to(device)\n",
        "\n",
        "        # Se obtiene el índice que finaliza la inferencia\n",
        "        eos = word2idx_outputs['<eos>']\n",
        "\n",
        "        output_sentence = []\n",
        "        for _ in range(max_out_len):\n",
        "            # Predicción del próximo elemento\n",
        "            output_tokens, h, c = best_decoder(target_seq, h, c)\n",
        "\n",
        "            # Obtener el índice con mayor probabilidad\n",
        "            idx = output_tokens[0, 0, :].argmax().item()\n",
        "\n",
        "            # Si es \"end of sentence <eos>\" se acaba\n",
        "            if eos == idx:\n",
        "                break\n",
        "\n",
        "            # Transformar idx a palabra\n",
        "            if idx > 0:\n",
        "                word = idx2word_target[idx]\n",
        "                output_sentence.append(word)\n",
        "\n",
        "            # Actualizar secuencia de entrada con la salida (re-alimentación)\n",
        "            target_seq = torch.LongTensor([[idx]]).to(device)\n",
        "\n",
        "        return ' '.join(output_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzJTx57Lfha9"
      },
      "outputs": [],
      "source": [
        "def translate_sentence_sampling(input_seq, temperature=1.0, max_repeat=3):\n",
        "    best_encoder.eval()\n",
        "    best_decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.LongTensor(input_seq).to(device)\n",
        "        h, c = best_encoder(input_tensor)\n",
        "\n",
        "        target_seq = torch.LongTensor([[word2idx_outputs['<sos>']]]).to(device)\n",
        "        eos = word2idx_outputs['<eos>']\n",
        "\n",
        "        output_sentence = []\n",
        "        last_idx = None\n",
        "        repeat_count = 0\n",
        "\n",
        "        for _ in range(max_out_len):\n",
        "            output_tokens, h, c = best_decoder(target_seq, h, c)\n",
        "\n",
        "            logits = output_tokens[0, 0, :] / temperature\n",
        "            logits = logits - logits.max()  # estabilidad numérica\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "            idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            if eos == idx:\n",
        "                break\n",
        "\n",
        "            # Si se repite el mismo token muchas veces, forzar corte\n",
        "            if idx == last_idx:\n",
        "                repeat_count += 1\n",
        "                if repeat_count >= max_repeat:\n",
        "                    break\n",
        "            else:\n",
        "                repeat_count = 0\n",
        "            last_idx = idx\n",
        "\n",
        "            # Si el token es <sos> o <pad>, ignorar y no agregar a la secuencia\n",
        "            if idx > 0 and idx != word2idx_outputs.get('<sos>', -1):\n",
        "                word = idx2word_target.get(idx, '')\n",
        "                output_sentence.append(word)\n",
        "\n",
        "            target_seq = torch.LongTensor([[idx]]).to(device)\n",
        "\n",
        "        return ' '.join(output_sentence[:max_out_len])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4wadi92fha9"
      },
      "outputs": [],
      "source": [
        "def translate_sentence_beam_search_stochastic(input_seq, beam_width=3, temperature=1.0):\n",
        "    best_encoder.eval()\n",
        "    best_decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.LongTensor(input_seq).to(device)\n",
        "        h, c = best_encoder(input_tensor)\n",
        "\n",
        "        sequences = [(['<sos>'], 0.0, (h, c))]  # (sequence, score, states)\n",
        "\n",
        "        for _ in range(max_out_len):\n",
        "            all_candidates = []\n",
        "            for seq, score, states in sequences:\n",
        "                h_state, c_state = states\n",
        "\n",
        "                target_seq = torch.LongTensor([[word2idx_outputs.get(seq[-1], 0)]]).to(device)\n",
        "\n",
        "                output_tokens, h_new, c_new = best_decoder(target_seq, h_state, c_state)\n",
        "\n",
        "                logits = output_tokens[0, 0, :] / temperature\n",
        "                logits = logits - logits.max()  # estabilidad numérica\n",
        "                probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "                # Obtener las top beam_width predicciones\n",
        "                top_probs, top_indices = torch.topk(probs, beam_width)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    idx = top_indices[i].item()\n",
        "                    word = idx2word_target.get(idx, '')\n",
        "                    candidate = (seq + [word], score - torch.log(top_probs[i]).item(), (h_new, c_new))\n",
        "                    all_candidates.append(candidate)\n",
        "\n",
        "            # Ordenar todas las secuencias candidatas por score y quedarse con las mejores beam_width\n",
        "            ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
        "            sequences = ordered[:beam_width]\n",
        "\n",
        "        # Seleccionar la secuencia con el mejor score que no termine en <eos>\n",
        "        best_sequence = sequences[0][0]\n",
        "        if '<eos>' in best_sequence:\n",
        "            best_sequence = best_sequence[:best_sequence.index('<eos>')]\n",
        "\n",
        "        return ' '.join(best_sequence[1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLqJpHvkfha9"
      },
      "outputs": [],
      "source": [
        "i = np.random.choice(len(input_sentences))\n",
        "input_seq = encoder_input_sequences[i:i+1]\n",
        "translation = translate_sentence(input_seq)\n",
        "print('-')\n",
        "print('Input:', input_sentences[i])\n",
        "print('Response:', translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IhHIIGwfha9"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de uso con muestreo aleatorio\n",
        "i = np.random.choice(len(input_sentences))\n",
        "input_seq = encoder_input_sequences[i:i+1]\n",
        "#translation = translate_sentence_sampling(input_seq, temperature=0.2)\n",
        "translation = translate_sentence_beam_search_stochastic(input_seq, temperature=0.8)\n",
        "print('-')\n",
        "print('Input:', input_sentences[i])\n",
        "print('Response (sampling):', translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZsugcBxfha-"
      },
      "outputs": [],
      "source": [
        "def generate_translations(input_test=\"\", temperature=1.0):\n",
        "    if input_test == \"\":\n",
        "        # Seleccionar una oración al azar del dataset\n",
        "        i = np.random.choice(len(input_sentences))\n",
        "        encoder_sequence_test = encoder_input_sequences[i:i+1]\n",
        "        print('Input:', input_sentences[i])\n",
        "    else:\n",
        "        # Procesar la oración de entrada\n",
        "        integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "        print(\"Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "        encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "        print(\"Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "        print('Input:', input_test)\n",
        "\n",
        "    translation = translate_sentence(encoder_sequence_test)\n",
        "    translation_2 = translate_sentence_beam_search_stochastic(encoder_sequence_test, temperature=temperature)\n",
        "\n",
        "    print('Response:', translation)\n",
        "    print('Response (Beam Search):', translation_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUB3gXsWfha-"
      },
      "outputs": [],
      "source": [
        "input_test = \"My mother say hi.\"\n",
        "generate_translations(input_test=input_test, temperature=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE0PiZXYfha-"
      },
      "outputs": [],
      "source": [
        "input_test = \"Every end is a new beginning\"\n",
        "generate_translations(input_test=input_test, temperature=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UaBfGDTfha-"
      },
      "outputs": [],
      "source": [
        "input_test = \"The best of both worlds\"\n",
        "generate_translations(input_test=input_test, temperature=1.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgswjacefha-"
      },
      "outputs": [],
      "source": [
        "input_test = \"I know what you mean\"\n",
        "generate_translations(input_test=input_test, temperature=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-iabau5fha-"
      },
      "outputs": [],
      "source": [
        "input_test = \"Give me a break\"\n",
        "generate_translations(input_test=input_test, temperature=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwEw1WnLfha-"
      },
      "outputs": [],
      "source": [
        "input_test = \"where there is a will there is a way\"\n",
        "generate_translations(input_test=input_test, temperature=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHS6FxCTfha-"
      },
      "source": [
        "### 6 - Conclusión\n",
        "\n",
        "A primera vista parece que el modelo tendría que funcionar muy bien por el accuracy alcanzado. La realidad es que las respuestas no tienen que ver demasiado con la pregunta/traducción pero la respuesta en si tiene bastante coherencia.\n",
        "\n",
        "**Solución implementada - Dataset de PyTorch:**\n",
        "\n",
        "Se implementó un `Dataset` basado en `torch.utils.data.Dataset` que:\n",
        "- Genera los batches on-the-fly durante el entrenamiento\n",
        "- PyTorch CrossEntropyLoss trabaja directamente con índices (sin necesidad de one-hot)\n",
        "- Permite entrenar con datasets mucho más grandes sin agotar la RAM\n",
        "- Aumentamos de 10,000 a 118,964 muestras de entrenamiento\n",
        "\n",
        "Otras mejoras posibles:\n",
        "- Transfer learning evitando tener que entrenar todo el modelo desde cero\n",
        "- Utilizar embeddings pre-entrenados para español también\n",
        "- Aumentar aún más el dataset aprovechando el DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQgfsVwxfha-"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
