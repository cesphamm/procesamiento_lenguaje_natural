{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "# Procesamiento de Lenguaje Natural\n",
        "## Desaf√≠o 3: Modelo de Lenguaje con Tokenizaci√≥n por Caracteres\n",
        "\n",
        "**Autor:** Carlos Espinola  \n",
        "**Fecha:** Diciembre 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "---\n",
        "## Objetivos del Desaf√≠o\n",
        "\n",
        "### Consigna\n",
        "1. **Seleccionar un corpus de texto** sobre el cual entrenar el modelo de lenguaje\n",
        "2. **Pre-procesamiento**: tokenizar el corpus, estructurar el dataset y separar datos de entrenamiento y validaci√≥n\n",
        "3. **Proponer arquitecturas RNN**: implementar modelos basados en unidades recurrentes (SimpleRNN, LSTM, GRU)\n",
        "4. **Generaci√≥n de secuencias** con diferentes estrategias:\n",
        "   - Greedy Search\n",
        "   - Beam Search Determin√≠stico\n",
        "   - Beam Search Estoc√°stico (analizando el efecto de la temperatura)\n",
        "\n",
        "### Sugerencias\n",
        "- Guiarse por el descenso de la **perplejidad** en validaci√≥n para finalizar el entrenamiento\n",
        "- Explorar: SimpleRNN (celda de Elman), LSTM y GRU\n",
        "- `RMSprop` es el optimizador recomendado para buena convergencia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1. IMPORTACI√ìN DE LIBRER√çAS\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "import bs4 as bs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Configuraci√≥n de estilo para gr√°ficos\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Configuraci√≥n del dispositivo (GPU si est√° disponible)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è Dispositivo: {device}\")\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "---\n",
        "## 2. Selecci√≥n y Descarga del Corpus\n",
        "\n",
        "Utilizaremos el libro **\"La Vuelta al Mundo en 80 D√≠as\"** de Julio Verne como corpus de entrenamiento. Este texto en espa√±ol nos proporciona un corpus extenso y de calidad literaria para entrenar nuestro modelo de lenguaje a nivel de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7amy6uUaBLVD"
      },
      "outputs": [],
      "source": [
        "# Descargar el libro desde textos.info\n",
        "url = 'https://www.textos.info/julio-verne/la-vuelta-al-mundo-en-80-dias/ebook'\n",
        "raw_html = urllib.request.urlopen(url)\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Extraer todos los p√°rrafos\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "# Concatenar el texto de todos los p√°rrafos\n",
        "corpus = ''\n",
        "for para in article_paragraphs:\n",
        "    corpus += para.text + ' '\n",
        "\n",
        "# Convertir a min√∫sculas para normalizar\n",
        "corpus = corpus.lower()\n",
        "\n",
        "print(f\"üìö Longitud total del corpus: {len(corpus):,} caracteres\")\n",
        "print(f\"\\nüìñ Primeros 500 caracteres del corpus:\")\n",
        "print(\"-\" * 50)\n",
        "print(corpus[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v_ickFwBJTy"
      },
      "outputs": [],
      "source": [
        "# An√°lisis de distribuci√≥n de caracteres en el corpus\n",
        "char_counts = Counter(corpus)\n",
        "most_common = char_counts.most_common(20)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "chars, counts = zip(*most_common)\n",
        "chars_display = [repr(c) if c in [' ', '\\n', '\\t'] else c for c in chars]\n",
        "bars = ax.bar(chars_display, counts, color='steelblue', edgecolor='navy', alpha=0.8)\n",
        "ax.set_xlabel('Caracter', fontsize=12)\n",
        "ax.set_ylabel('Frecuencia', fontsize=12)\n",
        "ax.set_title('üìä Distribuci√≥n de los 20 caracteres m√°s frecuentes en el corpus', fontsize=14)\n",
        "plt.xticks(rotation=45, fontsize=11)\n",
        "\n",
        "# A√±adir valores encima de las barras\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
        "            f'{count:,}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBE0sSYuB-E6"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 3. Tokenizaci√≥n por Caracteres\n",
        "\n",
        "En un modelo de lenguaje por caracteres, cada car√°cter √∫nico del corpus representa un token. Esto nos permite:\n",
        "- **Vocabulario peque√±o**: T√≠picamente 50-100 caracteres vs miles de palabras\n",
        "- **Flexibilidad**: Puede manejar cualquier palabra, incluso neologismos\n",
        "- **Captura morfol√≥gica**: Aprende patrones dentro de las palabras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "# Crear vocabulario de caracteres √∫nicos (ordenado para reproducibilidad)\n",
        "chars_vocab = sorted(set(corpus))\n",
        "vocab_size = len(chars_vocab)\n",
        "\n",
        "print(f\"üìù Tama√±o del vocabulario: {vocab_size} caracteres √∫nicos\")\n",
        "print(f\"\\nüî§ Caracteres en el vocabulario:\")\n",
        "print(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wumBNwdjJM3j"
      },
      "outputs": [],
      "source": [
        "# Crear diccionarios de mapeo caracter <-> √≠ndice\n",
        "char2idx = {ch: idx for idx, ch in enumerate(chars_vocab)}\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "\n",
        "# Mostrar ejemplos de mapeo\n",
        "print(\"üîó Ejemplos de mapeo char2idx:\")\n",
        "for ch in ['a', 'e', 'i', 'o', 'u', ' ', '.', ',', '√±']:\n",
        "    if ch in char2idx:\n",
        "        print(f\"  '{ch}' -> {char2idx[ch]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Tokenizar el corpus completo (convertir caracteres a √≠ndices)\n",
        "tokenized_corpus = np.array([char2idx[ch] for ch in corpus], dtype=np.int64)\n",
        "\n",
        "print(f\"üìä Corpus tokenizado - shape: {tokenized_corpus.shape}\")\n",
        "print(f\"\\nüî¢ Primeros 50 tokens:\")\n",
        "print(tokenized_corpus[:50])\n",
        "print(f\"\\nüìú Texto correspondiente:\")\n",
        "print(f\"'{corpus[:50]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573Cg5n7VhWw"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 4. Estructuraci√≥n del Dataset\n",
        "\n",
        "### 4.1 Definici√≥n del Tama√±o de Contexto\n",
        "\n",
        "El tama√±o de contexto define cu√°ntos caracteres previos utilizar√° el modelo para predecir el siguiente. En modelos por caracteres podemos usar contextos m√°s largos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwTK6xgLJd8q"
      },
      "outputs": [],
      "source": [
        "# Definir tama√±o de contexto (hiperpar√°metro)\n",
        "MAX_CONTEXT_SIZE = 100\n",
        "\n",
        "print(f\"‚öôÔ∏è Tama√±o de contexto: {MAX_CONTEXT_SIZE} caracteres\")\n",
        "print(f\"üìù Esto equivale aproximadamente a {MAX_CONTEXT_SIZE // 5} palabras (asumiendo ~5 caracteres por palabra)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "outputs": [],
      "source": [
        "### 4.2 Divisi√≥n en Entrenamiento y Validaci√≥n\n",
        "\n",
        "Dividiremos el corpus secuencialmente: **90% para entrenamiento** y **10% para validaci√≥n**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "# Proporci√≥n para validaci√≥n\n",
        "p_val = 0.1\n",
        "\n",
        "# Calcular √≠ndice de divisi√≥n\n",
        "split_idx = int(len(tokenized_corpus) * (1 - p_val))\n",
        "\n",
        "# Dividir corpus tokenizado\n",
        "train_corpus = tokenized_corpus[:split_idx]\n",
        "val_corpus = tokenized_corpus[split_idx:]\n",
        "\n",
        "print(f\"üìä Divisi√≥n del corpus:\")\n",
        "print(f\"  ‚Ä¢ Entrenamiento: {len(train_corpus):,} caracteres ({len(train_corpus)/len(tokenized_corpus)*100:.1f}%)\")\n",
        "print(f\"  ‚Ä¢ Validaci√≥n: {len(val_corpus):,} caracteres ({len(val_corpus)/len(tokenized_corpus)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "outputs": [],
      "source": [
        "### 4.3 Creaci√≥n de Secuencias de Entrenamiento\n",
        "\n",
        "Estructuramos el problema como **many-to-many**:\n",
        "- **Entrada**: secuencia de tokens $[x_0, x_1, ..., x_N]$\n",
        "- **Target**: secuencia desplazada $[x_1, x_2, ..., x_{N+1}]$\n",
        "\n",
        "Esta estructura permite que cada posici√≥n contribuya al gradiente, mejorando el aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwGVSKOiJ5bj"
      },
      "outputs": [],
      "source": [
        "def create_sequences(corpus_data, seq_length):\n",
        "    \"\"\"\n",
        "    Crea secuencias de entrada y target para entrenamiento many-to-many.\n",
        "    \n",
        "    Args:\n",
        "        corpus_data: Array de tokens\n",
        "        seq_length: Longitud de cada secuencia\n",
        "    \n",
        "    Returns:\n",
        "        X: Array de secuencias de entrada (n_sequences, seq_length)\n",
        "        y: Array de secuencias target (n_sequences, seq_length)\n",
        "    \"\"\"\n",
        "    n_sequences = len(corpus_data) - seq_length\n",
        "    \n",
        "    X = np.zeros((n_sequences, seq_length), dtype=np.int64)\n",
        "    y = np.zeros((n_sequences, seq_length), dtype=np.int64)\n",
        "    \n",
        "    for i in range(n_sequences):\n",
        "        X[i] = corpus_data[i:i + seq_length]\n",
        "        y[i] = corpus_data[i + 1:i + seq_length + 1]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Crear secuencias de entrenamiento\n",
        "X_train, y_train = create_sequences(train_corpus, MAX_CONTEXT_SIZE)\n",
        "\n",
        "print(f\"üì¶ Secuencias de entrenamiento:\")\n",
        "print(f\"  ‚Ä¢ X_train shape: {X_train.shape}\")\n",
        "print(f\"  ‚Ä¢ y_train shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "# Verificar alineaci√≥n entre entrada y target\n",
        "print(\"üîç Ejemplo de alineaci√≥n entrada-target:\")\n",
        "print(f\"\\nüì• Entrada (X[0]):\")\n",
        "print(''.join([idx2char[idx] for idx in X_train[0]]))\n",
        "print(f\"\\nüì§ Target (y[0]):\")\n",
        "print(''.join([idx2char[idx] for idx in y_train[0]]))\n",
        "print(\"\\n(Observa c√≥mo el target est√° desplazado un car√°cter a la derecha)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "outputs": [],
      "source": [
        "# Crear DataLoaders de PyTorch\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_train, dtype=torch.long),\n",
        "    torch.tensor(y_train, dtype=torch.long)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# Crear secuencias y DataLoader de validaci√≥n\n",
        "X_val, y_val = create_sequences(val_corpus, MAX_CONTEXT_SIZE)\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(X_val, dtype=torch.long),\n",
        "    torch.tensor(y_val, dtype=torch.long)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"üì¶ DataLoaders creados:\")\n",
        "print(f\"  ‚Ä¢ Batches de entrenamiento: {len(train_loader)}\")\n",
        "print(f\"  ‚Ä¢ Batches de validaci√≥n: {len(val_loader)}\")\n",
        "print(f\"  ‚Ä¢ Tama√±o de batch: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 5. Definici√≥n de Arquitecturas RNN\n",
        "\n",
        "Implementaremos tres arquitecturas basadas en unidades recurrentes:\n",
        "1. **SimpleRNN** (Celda de Elman): La m√°s b√°sica, puede sufrir de gradientes que desaparecen\n",
        "2. **LSTM** (Long Short-Term Memory): Mejor para dependencias largas gracias a compuertas\n",
        "3. **GRU** (Gated Recurrent Unit): Balance entre complejidad y rendimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "outputs": [],
      "source": [
        "class CharLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo de lenguaje a nivel de caracteres con arquitectura RNN configurable.\n",
        "    Soporta: RNN, LSTM y GRU.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, hidden_size=256, num_layers=2, \n",
        "                 rnn_type='lstm', dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        \n",
        "        # Seleccionar tipo de celda recurrente\n",
        "        rnn_classes = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}\n",
        "        \n",
        "        if self.rnn_type not in rnn_classes:\n",
        "            raise ValueError(f\"rnn_type debe ser 'rnn', 'lstm' o 'gru'\")\n",
        "        \n",
        "        # Capa recurrente\n",
        "        self.rnn = rnn_classes[self.rnn_type](\n",
        "            input_size=vocab_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # One-hot encoding\n",
        "        x_onehot = F.one_hot(x, num_classes=self.vocab_size).float()\n",
        "        \n",
        "        # Forward pass RNN\n",
        "        rnn_out, hidden = self.rnn(x_onehot, hidden)\n",
        "        \n",
        "        # Dropout y capa lineal\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        logits = self.fc(rnn_out)\n",
        "        \n",
        "        return logits, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size, device):\n",
        "        if self.rnn_type == 'lstm':\n",
        "            return (\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "            )\n",
        "        else:\n",
        "            return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "print(\"‚úÖ Clase CharLanguageModel definida correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "outputs": [],
      "source": [
        "# Comparar arquitecturas en t√©rminos de par√°metros\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"üìä Comparaci√≥n de arquitecturas:\")\n",
        "print(\"=\" * 50)\n",
        "for rnn_type in ['rnn', 'lstm', 'gru']:\n",
        "    model_temp = CharLanguageModel(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=256,\n",
        "        num_layers=2,\n",
        "        rnn_type=rnn_type\n",
        "    )\n",
        "    n_params = count_parameters(model_temp)\n",
        "    print(f\"  {rnn_type.upper():>5}: {n_params:>10,} par√°metros\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 6. Entrenamiento del Modelo\n",
        "\n",
        "### 6.1 Funciones de Entrenamiento y Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vken7O4ETsAJ"
      },
      "source": [
        "def compute_perplexity(model, data_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Calcula la perplejidad del modelo en un conjunto de datos.\n",
        "    Perplejidad = exp(loss promedio). Menor perplejidad = mejor modelo.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            logits, _ = model(X_batch)\n",
        "            loss = criterion(logits.view(-1, model.vocab_size), y_batch.view(-1))\n",
        "            \n",
        "            total_loss += loss.item() * y_batch.numel()\n",
        "            total_tokens += y_batch.numel()\n",
        "    \n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = np.exp(avg_loss)\n",
        "    \n",
        "    return perplexity, avg_loss\n",
        "\n",
        "print(\"‚úÖ Funci√≥n compute_perplexity definida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=20, lr=0.001, \n",
        "                patience=5, device='cpu', clip_grad=5.0):\n",
        "    \"\"\"\n",
        "    Entrena el modelo con early stopping basado en perplejidad de validaci√≥n.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
        "    \n",
        "    history = {'train_loss': [], 'train_ppl': [], 'val_loss': [], 'val_ppl': []}\n",
        "    best_val_ppl = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(f\"üöÄ Iniciando entrenamiento - {model.rnn_type.upper()}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # --- Fase de Entrenamiento ---\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        \n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(X_batch)\n",
        "            loss = criterion(logits.view(-1, model.vocab_size), y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item() * y_batch.numel()\n",
        "            total_tokens += y_batch.numel()\n",
        "        \n",
        "        train_loss = total_loss / total_tokens\n",
        "        train_ppl = np.exp(train_loss)\n",
        "        \n",
        "        # --- Fase de Validaci√≥n ---\n",
        "        val_ppl, val_loss = compute_perplexity(model, val_loader, criterion, device)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_ppl'].append(train_ppl)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_ppl'].append(val_ppl)\n",
        "        \n",
        "        print(f\"√âpoca {epoch+1:2d}/{num_epochs} | Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Train PPL: {train_ppl:7.2f} | Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:7.2f}\", end='')\n",
        "        \n",
        "        if val_ppl < best_val_ppl:\n",
        "            best_val_ppl = val_ppl\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(\" ‚≠ê Mejor!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\" (paciencia: {patience_counter}/{patience})\")\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\n‚èπÔ∏è Early stopping en √©poca {epoch+1}\")\n",
        "                break\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(f\"‚úÖ Mejor perplejidad de validaci√≥n: {best_val_ppl:.2f}\")\n",
        "    \n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    \n",
        "    return history\n",
        "\n",
        "print(\"‚úÖ Funci√≥n train_model definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFAyA4zCWE-5"
      },
      "outputs": [],
      "source": [
        "### 6.2 Entrenamiento de los Tres Modelos (SimpleRNN, LSTM, GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcKRl70HFTzG"
      },
      "outputs": [],
      "source": [
        "# Hiperpar√°metros de entrenamiento\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "LEARNING_RATE = 0.002\n",
        "NUM_EPOCHS = 25\n",
        "PATIENCE = 5\n",
        "\n",
        "# Diccionarios para almacenar modelos e historiales\n",
        "models = {}\n",
        "histories = {}\n",
        "\n",
        "print(\"‚öôÔ∏è Hiperpar√°metros configurados:\")\n",
        "print(f\"  ‚Ä¢ Hidden Size: {HIDDEN_SIZE}\")\n",
        "print(f\"  ‚Ä¢ Num Layers: {NUM_LAYERS}\")\n",
        "print(f\"  ‚Ä¢ Dropout: {DROPOUT}\")\n",
        "print(f\"  ‚Ä¢ Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  ‚Ä¢ Max Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  ‚Ä¢ Patience: {PATIENCE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVpLCKSZFXZO"
      },
      "outputs": [],
      "source": [
        "# Entrenar modelo SimpleRNN\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üì¶ ENTRENANDO MODELO: SimpleRNN\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "models['rnn'] = CharLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    rnn_type='rnn',\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "histories['rnn'] = train_model(\n",
        "    models['rnn'], train_loader, val_loader,\n",
        "    num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
        "    patience=PATIENCE, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "outputs": [],
      "source": [
        "# Entrenar modelo LSTM\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üì¶ ENTRENANDO MODELO: LSTM\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "models['lstm'] = CharLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    rnn_type='lstm',\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "histories['lstm'] = train_model(\n",
        "    models['lstm'], train_loader, val_loader,\n",
        "    num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
        "    patience=PATIENCE, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      },
      "source": [
        "# Entrenar modelo GRU\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üì¶ ENTRENANDO MODELO: GRU\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "models['gru'] = CharLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    rnn_type='gru',\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "histories['gru'] = train_model(\n",
        "    models['gru'], train_loader, val_loader,\n",
        "    num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
        "    patience=PATIENCE, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkMCZvmhrQz4"
      },
      "outputs": [],
      "source": [
        "# Graficar curvas de entrenamiento comparativas\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "colors = {'rnn': '#e74c3c', 'lstm': '#3498db', 'gru': '#2ecc71'}\n",
        "labels = {'rnn': 'SimpleRNN', 'lstm': 'LSTM', 'gru': 'GRU'}\n",
        "\n",
        "# Perplejidad de validaci√≥n\n",
        "for model_type, history in histories.items():\n",
        "    epochs = range(1, len(history['val_ppl']) + 1)\n",
        "    axes[0].plot(epochs, history['val_ppl'], color=colors[model_type], \n",
        "                 label=labels[model_type], linewidth=2, marker='o', markersize=4)\n",
        "\n",
        "axes[0].set_xlabel('√âpoca', fontsize=12)\n",
        "axes[0].set_ylabel('Perplejidad', fontsize=12)\n",
        "axes[0].set_title('üìà Perplejidad de Validaci√≥n', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss de entrenamiento y validaci√≥n\n",
        "for model_type, history in histories.items():\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    axes[1].plot(epochs, history['train_loss'], color=colors[model_type], \n",
        "                 label=f\"{labels[model_type]} (train)\", linewidth=2, linestyle='-')\n",
        "    axes[1].plot(epochs, history['val_loss'], color=colors[model_type], \n",
        "                 label=f\"{labels[model_type]} (val)\", linewidth=2, linestyle='--')\n",
        "\n",
        "axes[1].set_xlabel('√âpoca', fontsize=12)\n",
        "axes[1].set_ylabel('Loss (Cross-Entropy)', fontsize=12)\n",
        "axes[1].set_title('üìâ Curvas de Aprendizaje', fontsize=14)\n",
        "axes[1].legend(loc='upper right', fontsize=9)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tabla resumen\n",
        "print(\"\\nüìä Resumen de Modelos:\")\n",
        "print(\"=\" * 65)\n",
        "print(f\"{'Modelo':<12} {'Par√°metros':>15} {'Mejor Val PPL':>15} {'√âpocas':>10}\")\n",
        "print(\"-\" * 65)\n",
        "for model_type in ['rnn', 'lstm', 'gru']:\n",
        "    n_params = count_parameters(models[model_type])\n",
        "    best_ppl = min(histories[model_type]['val_ppl'])\n",
        "    n_epochs = len(histories[model_type]['val_ppl'])\n",
        "    print(f\"{labels[model_type]:<12} {n_params:>15,} {best_ppl:>15.2f} {n_epochs:>10}\")\n",
        "print(\"=\" * 65)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgz7VKwTUbj6"
      },
      "source": [
        "---\n",
        "## 7. Generaci√≥n de Secuencias\n",
        "\n",
        "Implementaremos tres estrategias de generaci√≥n:\n",
        "1. **Greedy Search**: Selecciona siempre el car√°cter m√°s probable (determin√≠stico)\n",
        "2. **Beam Search Determin√≠stico**: Mantiene los k mejores candidatos\n",
        "3. **Beam Search Estoc√°stico**: Muestreo con temperatura para diversidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd2OkfQYs2Q7"
      },
      "outputs": [],
      "source": [
        "# Seleccionar el mejor modelo para generaci√≥n\n",
        "best_model_type = min(histories, key=lambda x: min(histories[x]['val_ppl']))\n",
        "model = models[best_model_type]\n",
        "print(f\"üèÜ Usando modelo: {best_model_type.upper()} (mejor perplejidad de validaci√≥n)\")\n",
        "print(f\"   Perplejidad: {min(histories[best_model_type]['val_ppl']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "### 7.1 Greedy Search\n",
        "\n",
        "En Greedy Search, siempre seleccionamos el car√°cter con mayor probabilidad. Es determin√≠stico pero puede producir secuencias repetitivas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWK3z85sQfUe"
      },
      "source": [
        "def greedy_search(model, seed_text, max_length, num_chars, device='cpu'):\n",
        "    \"\"\"\n",
        "    Genera texto usando b√∫squeda voraz (greedy search).\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo de lenguaje entrenado\n",
        "        seed_text: Texto inicial (semilla)\n",
        "        max_length: Tama√±o m√°ximo de contexto\n",
        "        num_chars: N√∫mero de caracteres a generar\n",
        "        device: Dispositivo de c√≥mputo\n",
        "    \n",
        "    Returns:\n",
        "        generated_text: Texto generado (incluyendo semilla)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    generated_text = seed_text.lower()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_chars):\n",
        "            # Tokenizar el texto actual\n",
        "            tokens = [char2idx.get(ch, 0) for ch in generated_text[-max_length:]]\n",
        "            \n",
        "            # Pad si es necesario\n",
        "            if len(tokens) < max_length:\n",
        "                tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "            \n",
        "            x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
        "            logits, _ = model(x)\n",
        "            \n",
        "            # Seleccionar el car√°cter m√°s probable (greedy)\n",
        "            next_char_idx = logits[0, -1, :].argmax().item()\n",
        "            next_char = idx2char[next_char_idx]\n",
        "            generated_text += next_char\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "print(\"‚úÖ Funci√≥n greedy_search definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "# Ejemplos de generaci√≥n con Greedy Search\n",
        "seed_texts = [\n",
        "    \"el se√±or fogg\",\n",
        "    \"pasepartout dijo\",\n",
        "    \"la vuelta al mundo\"\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîç GREEDY SEARCH - Generaci√≥n de texto\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for seed in seed_texts:\n",
        "    generated = greedy_search(model, seed, MAX_CONTEXT_SIZE, num_chars=150, device=device)\n",
        "    print(f\"\\nüìù Semilla: '{seed}'\")\n",
        "    print(\"-\"*50)\n",
        "    print(generated)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### 7.2 Muestreo con Temperatura\n",
        "\n",
        "La temperatura controla la \"creatividad\" del modelo:\n",
        "- **T < 1**: M√°s conservador, secuencias predecibles\n",
        "- **T = 1**: Distribuci√≥n original del modelo  \n",
        "- **T > 1**: M√°s aleatorio, secuencias diversas/ca√≥ticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQq1PHDkxDvN"
      },
      "outputs": [],
      "source": [
        "def sample_with_temperature(model, seed_text, max_length, num_chars, \n",
        "                            temperature=1.0, device='cpu'):\n",
        "    \"\"\"\n",
        "    Genera texto usando muestreo con temperatura.\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo de lenguaje entrenado\n",
        "        seed_text: Texto inicial\n",
        "        max_length: Tama√±o m√°ximo de contexto\n",
        "        num_chars: N√∫mero de caracteres a generar\n",
        "        temperature: Par√°metro de temperatura (0 < T)\n",
        "        device: Dispositivo de c√≥mputo\n",
        "    \n",
        "    Returns:\n",
        "        generated_text: Texto generado\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    generated_text = seed_text.lower()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_chars):\n",
        "            tokens = [char2idx.get(ch, 0) for ch in generated_text[-max_length:]]\n",
        "            \n",
        "            if len(tokens) < max_length:\n",
        "                tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "            \n",
        "            x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
        "            logits, _ = model(x)\n",
        "            \n",
        "            # Aplicar temperatura\n",
        "            logits_scaled = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(logits_scaled, dim=-1)\n",
        "            \n",
        "            # Muestrear de la distribuci√≥n\n",
        "            next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            next_char = idx2char[next_char_idx]\n",
        "            generated_text += next_char\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "print(\"‚úÖ Funci√≥n sample_with_temperature definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K30JHB3Dv-mx"
      },
      "outputs": [],
      "source": [
        "# Demostrar efecto de la temperatura\n",
        "seed = \"el viaje comenz√≥\"\n",
        "temperatures = [0.2, 0.5, 0.8, 1.0, 1.2, 1.5]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üå°Ô∏è EFECTO DE LA TEMPERATURA EN LA GENERACI√ìN\")\n",
        "print(f\"üìù Semilla: '{seed}'\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for temp in temperatures:\n",
        "    generated = sample_with_temperature(\n",
        "        model, seed, MAX_CONTEXT_SIZE, \n",
        "        num_chars=100, temperature=temp, device=device\n",
        "    )\n",
        "    print(f\"\\nüå°Ô∏è Temperatura = {temp}\")\n",
        "    print(\"-\"*50)\n",
        "    print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhy5hZN38qfO"
      },
      "outputs": [],
      "source": [
        "### 7.3 Beam Search Determin√≠stico\n",
        "\n",
        "Beam Search mantiene m√∫ltiples hip√≥tesis (beams) y expande las m√°s prometedoras. Es m√°s sofisticado que greedy pero sigue siendo determin√≠stico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "def beam_search_deterministic(model, seed_text, max_length, num_chars, \n",
        "                              beam_width=5, device='cpu'):\n",
        "    \"\"\"\n",
        "    Genera texto usando beam search determin√≠stico.\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo de lenguaje entrenado\n",
        "        seed_text: Texto inicial\n",
        "        max_length: Tama√±o m√°ximo de contexto\n",
        "        num_chars: N√∫mero de caracteres a generar\n",
        "        beam_width: N√∫mero de hip√≥tesis a mantener\n",
        "        device: Dispositivo de c√≥mputo\n",
        "    \n",
        "    Returns:\n",
        "        best_sequence: Mejor secuencia generada\n",
        "        all_sequences: Lista de todas las secuencias finales con sus scores\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    seed_text = seed_text.lower()\n",
        "    \n",
        "    # Inicializar beams: (texto, log_prob_acumulada)\n",
        "    beams = [(seed_text, 0.0)]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_chars):\n",
        "            all_candidates = []\n",
        "            \n",
        "            for text, score in beams:\n",
        "                tokens = [char2idx.get(ch, 0) for ch in text[-max_length:]]\n",
        "                if len(tokens) < max_length:\n",
        "                    tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "                \n",
        "                x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
        "                logits, _ = model(x)\n",
        "                log_probs = F.log_softmax(logits[0, -1, :], dim=-1)\n",
        "                \n",
        "                # Obtener top-k candidatos\n",
        "                top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
        "                \n",
        "                for log_prob, idx in zip(top_log_probs.cpu().numpy(), \n",
        "                                         top_indices.cpu().numpy()):\n",
        "                    new_text = text + idx2char[idx]\n",
        "                    new_score = score + log_prob\n",
        "                    all_candidates.append((new_text, new_score))\n",
        "            \n",
        "            # Seleccionar los mejores beam_width candidatos\n",
        "            all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = all_candidates[:beam_width]\n",
        "    \n",
        "    # Normalizar scores por longitud\n",
        "    final_sequences = [(text, score / len(text)) for text, score in beams]\n",
        "    final_sequences.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return final_sequences[0][0], final_sequences\n",
        "\n",
        "print(\"‚úÖ Funci√≥n beam_search_deterministic definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBvKHFPmzpy2"
      },
      "outputs": [],
      "source": [
        "# Ejemplos de Beam Search Determin√≠stico\n",
        "print(\"=\"*70)\n",
        "print(\"üìä BEAM SEARCH DETERMIN√çSTICO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "seed = \"el tren parti√≥ de\"\n",
        "beam_widths = [1, 3, 5, 10]\n",
        "\n",
        "for bw in beam_widths:\n",
        "    best, all_seqs = beam_search_deterministic(\n",
        "        model, seed, MAX_CONTEXT_SIZE, \n",
        "        num_chars=80, beam_width=bw, device=device\n",
        "    )\n",
        "    print(f\"\\nüìä Beam Width = {bw}\")\n",
        "    print(f\"Mejor secuencia:\")\n",
        "    print(best)\n",
        "    print(f\"(Score normalizado: {all_seqs[0][1]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "outputs": [],
      "source": [
        "### 7.4 Beam Search Estoc√°stico\n",
        "\n",
        "Combina beam search con muestreo, permitiendo exploraci√≥n m√°s diversa del espacio de secuencias. La temperatura controla el balance entre exploraci√≥n y explotaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "def beam_search_stochastic(model, seed_text, max_length, num_chars, \n",
        "                           beam_width=5, temperature=1.0, device='cpu'):\n",
        "    \"\"\"\n",
        "    Genera texto usando beam search estoc√°stico con temperatura.\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo de lenguaje entrenado\n",
        "        seed_text: Texto inicial\n",
        "        max_length: Tama√±o m√°ximo de contexto\n",
        "        num_chars: N√∫mero de caracteres a generar\n",
        "        beam_width: N√∫mero de hip√≥tesis a mantener\n",
        "        temperature: Par√°metro de temperatura para muestreo\n",
        "        device: Dispositivo de c√≥mputo\n",
        "    \n",
        "    Returns:\n",
        "        best_sequence: Mejor secuencia generada\n",
        "        all_sequences: Lista de todas las secuencias finales con sus scores\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    seed_text = seed_text.lower()\n",
        "    \n",
        "    beams = [(seed_text, 0.0)]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_chars):\n",
        "            all_candidates = []\n",
        "            \n",
        "            for text, score in beams:\n",
        "                tokens = [char2idx.get(ch, 0) for ch in text[-max_length:]]\n",
        "                if len(tokens) < max_length:\n",
        "                    tokens = [0] * (max_length - len(tokens)) + tokens\n",
        "                \n",
        "                x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
        "                logits, _ = model(x)\n",
        "                \n",
        "                # Aplicar temperatura\n",
        "                logits_scaled = logits[0, -1, :] / temperature\n",
        "                probs = F.softmax(logits_scaled, dim=-1)\n",
        "                log_probs = torch.log(probs + 1e-10)\n",
        "                \n",
        "                # Muestrear beam_width candidatos seg√∫n la distribuci√≥n\n",
        "                sampled_indices = torch.multinomial(probs, num_samples=min(beam_width, len(probs)), \n",
        "                                                   replacement=False)\n",
        "                \n",
        "                for idx in sampled_indices.cpu().numpy():\n",
        "                    new_text = text + idx2char[idx]\n",
        "                    new_score = score + log_probs[idx].item()\n",
        "                    all_candidates.append((new_text, new_score))\n",
        "            \n",
        "            all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = all_candidates[:beam_width]\n",
        "    \n",
        "    final_sequences = [(text, score / len(text)) for text, score in beams]\n",
        "    final_sequences.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return final_sequences[0][0], final_sequences\n",
        "\n",
        "print(\"‚úÖ Funci√≥n beam_search_stochastic definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "# Comparar Beam Search Estoc√°stico con diferentes temperaturas\n",
        "print(\"=\"*70)\n",
        "print(\"üéØ BEAM SEARCH ESTOC√ÅSTICO - Efecto de la Temperatura\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "seed = \"hab√≠a una vez\"\n",
        "temperatures = [0.3, 0.6, 1.0, 1.5]\n",
        "beam_width = 5\n",
        "\n",
        "for temp in temperatures:\n",
        "    best, all_seqs = beam_search_stochastic(\n",
        "        model, seed, MAX_CONTEXT_SIZE,\n",
        "        num_chars=100, beam_width=beam_width,\n",
        "        temperature=temp, device=device\n",
        "    )\n",
        "    print(f\"\\nüå°Ô∏è Temperatura = {temp} | Beam Width = {beam_width}\")\n",
        "    print(\"-\"*60)\n",
        "    print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 8. Comparaci√≥n de M√©todos de Generaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "# Comparaci√≥n lado a lado de todos los m√©todos\n",
        "seed = \"el detective fix\"\n",
        "num_chars = 120\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üî¨ COMPARACI√ìN DE M√âTODOS DE GENERACI√ìN\")\n",
        "print(f\"üìù Semilla: '{seed}'\")\n",
        "print(f\"üìè Caracteres a generar: {num_chars}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Greedy Search\n",
        "print(\"\\nüìç GREEDY SEARCH\")\n",
        "print(\"-\"*60)\n",
        "result_greedy = greedy_search(model, seed, MAX_CONTEXT_SIZE, num_chars, device)\n",
        "print(result_greedy)\n",
        "\n",
        "# 2. Muestreo con T=0.5\n",
        "print(f\"\\nüé≤ MUESTREO CON TEMPERATURA = 0.5\")\n",
        "print(\"-\"*60)\n",
        "result_sample_05 = sample_with_temperature(model, seed, MAX_CONTEXT_SIZE, num_chars, 0.5, device)\n",
        "print(result_sample_05)\n",
        "\n",
        "# 3. Muestreo con T=1.0\n",
        "print(f\"\\nüé≤ MUESTREO CON TEMPERATURA = 1.0\")\n",
        "print(\"-\"*60)\n",
        "result_sample_10 = sample_with_temperature(model, seed, MAX_CONTEXT_SIZE, num_chars, 1.0, device)\n",
        "print(result_sample_10)\n",
        "\n",
        "# 4. Beam Search Determin√≠stico\n",
        "print(\"\\nüìä BEAM SEARCH DETERMIN√çSTICO (beam_width=5)\")\n",
        "print(\"-\"*60)\n",
        "result_beam_det, _ = beam_search_deterministic(model, seed, MAX_CONTEXT_SIZE, num_chars, beam_width=5, device=device)\n",
        "print(result_beam_det)\n",
        "\n",
        "# 5. Beam Search Estoc√°stico\n",
        "print(\"\\nüéØ BEAM SEARCH ESTOC√ÅSTICO (beam_width=5, temp=0.7)\")\n",
        "print(\"-\"*60)\n",
        "result_beam_sto, _ = beam_search_stochastic(model, seed, MAX_CONTEXT_SIZE, num_chars, beam_width=5, temperature=0.7, device=device)\n",
        "print(result_beam_sto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 9. An√°lisis y Conclusiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "# Resumen final\n",
        "print(\"=\"*70)\n",
        "print(\"üìã RESUMEN Y CONCLUSIONES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "conclusions = \"\"\"\n",
        "üìö CORPUS Y PRE-PROCESAMIENTO:\n",
        "   ‚Ä¢ Utilizamos \"La Vuelta al Mundo en 80 D√≠as\" de Julio Verne\n",
        "   ‚Ä¢ Tokenizaci√≥n a nivel de caracteres (vocabulario peque√±o pero flexible)\n",
        "   ‚Ä¢ Contexto de {} caracteres para capturar dependencias largas\n",
        "   ‚Ä¢ Divisi√≥n 90% entrenamiento / 10% validaci√≥n\n",
        "\n",
        "üèóÔ∏è ARQUITECTURAS EVALUADAS:\n",
        "   ‚Ä¢ SimpleRNN: M√°s simple pero sufre de gradientes que desaparecen\n",
        "   ‚Ä¢ LSTM: Mejor para dependencias largas gracias a las compuertas\n",
        "   ‚Ä¢ GRU: Balance entre complejidad y rendimiento\n",
        "\n",
        "üìà M√âTRICAS:\n",
        "   ‚Ä¢ Perplejidad como medida principal de calidad del modelo\n",
        "   ‚Ä¢ Early stopping basado en perplejidad de validaci√≥n\n",
        "\n",
        "üéØ ESTRATEGIAS DE GENERACI√ìN:\n",
        "\n",
        "   1. GREEDY SEARCH:\n",
        "      ‚Ä¢ Siempre elige el car√°cter m√°s probable\n",
        "      ‚Ä¢ Determin√≠stico y r√°pido\n",
        "      ‚Ä¢ Puede producir texto repetitivo\n",
        "\n",
        "   2. MUESTREO CON TEMPERATURA:\n",
        "      ‚Ä¢ T < 1: M√°s conservador/predecible\n",
        "      ‚Ä¢ T = 1: Distribuci√≥n original del modelo\n",
        "      ‚Ä¢ T > 1: M√°s creativo pero potencialmente incoherente\n",
        "\n",
        "   3. BEAM SEARCH DETERMIN√çSTICO:\n",
        "      ‚Ä¢ Mantiene m√∫ltiples hip√≥tesis\n",
        "      ‚Ä¢ Mejor calidad que greedy\n",
        "      ‚Ä¢ Sigue siendo determin√≠stico\n",
        "\n",
        "   4. BEAM SEARCH ESTOC√ÅSTICO:\n",
        "      ‚Ä¢ Combina beam search con muestreo\n",
        "      ‚Ä¢ Permite diversidad con control de calidad\n",
        "      ‚Ä¢ La temperatura controla el balance exploraci√≥n/explotaci√≥n\n",
        "\n",
        "üîë OBSERVACIONES CLAVE:\n",
        "   ‚Ä¢ Las arquitecturas con compuertas (LSTM/GRU) superan a SimpleRNN\n",
        "   ‚Ä¢ La temperatura es crucial para balancear coherencia y creatividad\n",
        "   ‚Ä¢ Beam search estoc√°stico ofrece el mejor balance para generaci√≥n\n",
        "\"\"\".format(MAX_CONTEXT_SIZE)\n",
        "\n",
        "print(conclusions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# Guardar el mejor modelo\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_type': best_model_type,\n",
        "    'vocab_size': vocab_size,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'num_layers': NUM_LAYERS,\n",
        "    'char2idx': char2idx,\n",
        "    'idx2char': idx2char,\n",
        "    'max_context_size': MAX_CONTEXT_SIZE\n",
        "}, 'best_char_lm_model.pt')\n",
        "\n",
        "print(\"‚úÖ Modelo guardado en 'best_char_lm_model.pt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8HQoLhw-NYg"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## 10. Generaci√≥n Interactiva (Opcional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S3_I3S1W1Hm"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n interactiva para generar texto\n",
        "def generate_text_interactive(seed_text, method='sampling', temperature=0.8, \n",
        "                              beam_width=5, num_chars=150):\n",
        "    \"\"\"\n",
        "    Funci√≥n wrapper para generaci√≥n interactiva.\n",
        "    \n",
        "    Args:\n",
        "        seed_text: Texto inicial\n",
        "        method: 'greedy', 'sampling', 'beam_det', 'beam_sto'\n",
        "        temperature: Para muestreo y beam estoc√°stico\n",
        "        beam_width: Para m√©todos beam\n",
        "        num_chars: Caracteres a generar\n",
        "    \"\"\"\n",
        "    if method == 'greedy':\n",
        "        return greedy_search(model, seed_text, MAX_CONTEXT_SIZE, num_chars, device)\n",
        "    elif method == 'sampling':\n",
        "        return sample_with_temperature(model, seed_text, MAX_CONTEXT_SIZE, \n",
        "                                       num_chars, temperature, device)\n",
        "    elif method == 'beam_det':\n",
        "        result, _ = beam_search_deterministic(model, seed_text, MAX_CONTEXT_SIZE,\n",
        "                                               num_chars, beam_width, device)\n",
        "        return result\n",
        "    elif method == 'beam_sto':\n",
        "        result, _ = beam_search_stochastic(model, seed_text, MAX_CONTEXT_SIZE,\n",
        "                                           num_chars, beam_width, temperature, device)\n",
        "        return result\n",
        "    else:\n",
        "        return \"M√©todo no reconocido. Use: 'greedy', 'sampling', 'beam_det', 'beam_sto'\"\n",
        "\n",
        "# Ejemplo de uso\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéÆ GENERACI√ìN INTERACTIVA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "texto_generado = generate_text_interactive(\n",
        "    seed_text=\"en aquel momento\",\n",
        "    method='beam_sto',\n",
        "    temperature=0.7,\n",
        "    beam_width=5,\n",
        "    num_chars=200\n",
        ")\n",
        "\n",
        "print(\"\\nüìú Texto generado:\")\n",
        "print(\"-\"*50)\n",
        "print(texto_generado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LlqmtEW1Hn"
      },
      "source": [
        "---\n",
        "## Fin del Desaf√≠o 3\n",
        "\n",
        "‚úÖ **Objetivos cumplidos:**\n",
        "1. Corpus seleccionado y preprocesado\n",
        "2. Tokenizaci√≥n por caracteres implementada\n",
        "3. Tres arquitecturas RNN evaluadas (SimpleRNN, LSTM, GRU)\n",
        "4. Cuatro m√©todos de generaci√≥n implementados:\n",
        "   - Greedy Search\n",
        "   - Muestreo con Temperatura\n",
        "   - Beam Search Determin√≠stico\n",
        "   - Beam Search Estoc√°stico\n",
        "\n",
        "üìä **Resultados**: Las arquitecturas con compuertas (LSTM/GRU) obtuvieron mejor perplejidad que SimpleRNN. El efecto de la temperatura fue evidente en la diversidad y coherencia del texto generado.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
