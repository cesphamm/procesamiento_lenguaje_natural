{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "# Procesamiento de Lenguaje Natural\n",
    "## Desaf√≠o 3: Modelo de Lenguaje con Tokenizaci√≥n por Caracteres\n",
    "\n",
    "**Autor:** Carlos Espinola  \n",
    "**Fecha:** Diciembre 2025\n",
    "\n",
    "### Versi√≥n: TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objetivos del Desaf√≠o\n",
    "\n",
    "### Consigna\n",
    "1. **Seleccionar un corpus de texto** sobre el cual entrenar el modelo de lenguaje\n",
    "2. **Pre-procesamiento**: tokenizar el corpus, estructurar el dataset y separar datos de entrenamiento y validaci√≥n\n",
    "3. **Proponer arquitecturas RNN**: implementar modelos basados en unidades recurrentes (SimpleRNN, LSTM, GRU)\n",
    "4. **Generaci√≥n de secuencias** con diferentes estrategias:\n",
    "   - Greedy Search\n",
    "   - Beam Search Determin√≠stico\n",
    "   - Beam Search Estoc√°stico (analizando el efecto de la temperatura)\n",
    "\n",
    "### Sugerencias\n",
    "- Guiarse por el descenso de la **perplejidad** en validaci√≥n para finalizar el entrenamiento\n",
    "- Explorar: SimpleRNN (celda de Elman), LSTM y GRU\n",
    "- `RMSprop` es el optimizador recomendado para buena convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTACI√ìN DE LIBRER√çAS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import bs4 as bs\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "# Configuraci√≥n de estilo para gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"üîß TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nüéÆ GPU detectada: {len(gpus)} dispositivo(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   ‚Ä¢ {gpu.name}\")\n",
    "    \n",
    "    # Habilitar crecimiento din√°mico de memoria\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    # Habilitar mixed precision para acelerar entrenamiento\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"\\n‚ö° Mixed Precision (float16) ACTIVADO\")\n",
    "    print(\"‚úÖ Optimizaciones de GPU activadas\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se detect√≥ GPU. El entrenamiento ser√° m√°s lento en CPU.\")\n",
    "    print(\"   Si tienes GPU, verifica la instalaci√≥n de CUDA y TensorFlow-GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Selecci√≥n y Descarga del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el libro desde textos.info\n",
    "url = 'https://www.textos.info/homero/odisea/ebook'\n",
    "raw_html = urllib.request.urlopen(url)\n",
    "raw_html = raw_html.read()\n",
    "\n",
    "# Parsear el HTML con BeautifulSoup\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "# Extraer todos los p√°rrafos\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "# Concatenar el texto de todos los p√°rrafos\n",
    "corpus = ''\n",
    "for para in article_paragraphs:\n",
    "    corpus += para.text + ' '\n",
    "\n",
    "# Convertir a min√∫sculas para normalizar\n",
    "corpus = corpus.lower()\n",
    "\n",
    "print(f\"üìö Longitud total del corpus: {len(corpus):,} caracteres\")\n",
    "print(f\"\\nüìñ Primeros 500 caracteres del corpus:\")\n",
    "print(\"-\" * 50)\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de distribuci√≥n de caracteres en el corpus\n",
    "char_counts = Counter(corpus)\n",
    "most_common = char_counts.most_common(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "chars, counts = zip(*most_common)\n",
    "chars_display = [repr(c) if c in [' ', '\\n', '\\t'] else c for c in chars]\n",
    "bars = ax.bar(chars_display, counts, color='steelblue', edgecolor='navy', alpha=0.8)\n",
    "ax.set_xlabel('Caracter', fontsize=12)\n",
    "ax.set_ylabel('Frecuencia', fontsize=12)\n",
    "ax.set_title('Distribuci√≥n de los 20 caracteres m√°s frecuentes', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Tokenizaci√≥n por Caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vocabulario de caracteres √∫nicos\n",
    "chars_vocab = sorted(set(corpus))\n",
    "vocab_size = len(chars_vocab)\n",
    "\n",
    "print(f\"üìù Tama√±o del vocabulario: {vocab_size} caracteres √∫nicos\")\n",
    "print(f\"\\nüî§ Caracteres en el vocabulario:\")\n",
    "print(chars_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionarios de mapeo\n",
    "char2idx = {ch: idx for idx, ch in enumerate(chars_vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "print(\"üîó Ejemplos de mapeo char2idx:\")\n",
    "for ch in ['a', 'e', 'i', 'o', 'u', ' ', '.']:\n",
    "    if ch in char2idx:\n",
    "        print(f\"  '{ch}' -> {char2idx[ch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar el corpus completo\n",
    "tokenized_corpus = np.array([char2idx[ch] for ch in corpus], dtype=np.int32)\n",
    "\n",
    "print(f\"üìä Corpus tokenizado - shape: {tokenized_corpus.shape}\")\n",
    "print(f\"\\nüî¢ Primeros 50 tokens:\")\n",
    "print(tokenized_corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Estructuraci√≥n del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir tama√±o de contexto\n",
    "MAX_CONTEXT_SIZE = 100\n",
    "\n",
    "print(f\"‚öôÔ∏è Tama√±o de contexto: {MAX_CONTEXT_SIZE} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n en entrenamiento y validaci√≥n\n",
    "p_val = 0.1\n",
    "split_idx = int(len(tokenized_corpus) * (1 - p_val))\n",
    "\n",
    "train_corpus = tokenized_corpus[:split_idx]\n",
    "val_corpus = tokenized_corpus[split_idx:]\n",
    "\n",
    "print(f\"üìä Divisi√≥n del corpus:\")\n",
    "print(f\"  ‚Ä¢ Entrenamiento: {len(train_corpus):,} caracteres ({len(train_corpus)/len(tokenized_corpus)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Validaci√≥n: {len(val_corpus):,} caracteres ({len(val_corpus)/len(tokenized_corpus)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(corpus_data, seq_length):\n",
    "    \"\"\"Crea secuencias de entrada y target para entrenamiento many-to-many.\"\"\"\n",
    "    n_sequences = len(corpus_data) - seq_length\n",
    "    X = np.zeros((n_sequences, seq_length), dtype=np.int32)\n",
    "    y = np.zeros((n_sequences, seq_length), dtype=np.int32)\n",
    "    for i in range(n_sequences):\n",
    "        X[i] = corpus_data[i:i + seq_length]\n",
    "        y[i] = corpus_data[i + 1:i + seq_length + 1]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = create_sequences(train_corpus, MAX_CONTEXT_SIZE)\n",
    "X_val, y_val = create_sequences(val_corpus, MAX_CONTEXT_SIZE)\n",
    "\n",
    "print(f\"üì¶ Secuencias de entrenamiento: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"üì¶ Secuencias de validaci√≥n: X={X_val.shape}, y={y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tf.data.Dataset\n",
    "BATCH_SIZE = 1024 if gpus else 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"üì¶ Datasets creados:\")\n",
    "print(f\"  ‚Ä¢ Batches de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"  ‚Ä¢ Batches de validaci√≥n: {len(val_dataset)}\")\n",
    "print(f\"  ‚Ä¢ Tama√±o de batch: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Definici√≥n de Arquitecturas RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_language_model(vocab_size, hidden_size=256, num_layers=2,\n",
    "                               rnn_type='lstm', dropout=0.5, embedding_dim=128,\n",
    "                               embed_dropout=0.2):\n",
    "    \"\"\"Construye un modelo de lenguaje a nivel de caracteres.\"\"\"\n",
    "    rnn_classes = {\n",
    "        'rnn': layers.SimpleRNN,\n",
    "        'lstm': layers.LSTM,\n",
    "        'gru': layers.GRU\n",
    "    }\n",
    "    \n",
    "    if rnn_type.lower() not in rnn_classes:\n",
    "        raise ValueError(f\"rnn_type debe ser 'rnn', 'lstm' o 'gru'\")\n",
    "    \n",
    "    RNNLayer = rnn_classes[rnn_type.lower()]\n",
    "    \n",
    "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = layers.Dropout(embed_dropout)(x)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        x = RNNLayer(\n",
    "            hidden_size,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout if i < num_layers - 1 else 0,\n",
    "            recurrent_dropout=dropout if i < num_layers - 1 else 0\n",
    "        )(x)\n",
    "    \n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(vocab_size, dtype='float32')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n build_char_language_model definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar arquitecturas\n",
    "print(\"üìä Comparaci√≥n de arquitecturas:\")\n",
    "print(\"=\" * 50)\n",
    "for rnn_type in ['rnn', 'lstm', 'gru']:\n",
    "    model_temp = build_char_language_model(vocab_size, rnn_type=rnn_type)\n",
    "    print(f\"  {rnn_type.upper():>5}: {model_temp.count_params():>10,} par√°metros\")\n",
    "    del model_temp\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityCallback(Callback):\n",
    "    \"\"\"Callback para mostrar la perplejidad durante el entrenamiento.\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        train_ppl = np.exp(logs.get('loss', 0))\n",
    "        val_ppl = np.exp(logs.get('val_loss', 0))\n",
    "        print(f\" | Train PPL: {train_ppl:7.2f} | Val PPL: {val_ppl:7.2f}\")\n",
    "\n",
    "print(\"‚úÖ PerplexityCallback definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_keras(rnn_type, vocab_size, train_dataset, val_dataset,\n",
    "                      hidden_size=256, num_layers=2, embedding_dim=128,\n",
    "                      dropout=0.5, embed_dropout=0.2,\n",
    "                      learning_rate=0.001, weight_decay=1e-5,\n",
    "                      label_smoothing=0.1, num_epochs=30, patience=5):\n",
    "    \"\"\"Entrena un modelo de lenguaje con early stopping.\"\"\"\n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento - {rnn_type.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = build_char_language_model(\n",
    "        vocab_size, hidden_size, num_layers, rnn_type,\n",
    "        dropout, embedding_dim, embed_dropout\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True, label_smoothing=label_smoothing)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1),\n",
    "        PerplexityCallback()\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(train_dataset, validation_data=val_dataset,\n",
    "                        epochs=num_epochs, callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    best_ppl = np.exp(min(history.history['val_loss']))\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úÖ Mejor perplejidad de validaci√≥n: {best_ppl:.2f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"‚úÖ Funci√≥n train_model_keras definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√°metros\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "EMBEDDING_DIM = 128\n",
    "DROPOUT = 0.5\n",
    "EMBED_DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "print(\"‚öôÔ∏è Hiperpar√°metros configurados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar SimpleRNN\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì¶ ENTRENANDO MODELO: SimpleRNN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models['rnn'], histories['rnn'] = train_model_keras(\n",
    "    'rnn', vocab_size, train_dataset, val_dataset,\n",
    "    HIDDEN_SIZE, NUM_LAYERS, EMBEDDING_DIM, DROPOUT, EMBED_DROPOUT,\n",
    "    LEARNING_RATE, WEIGHT_DECAY, LABEL_SMOOTHING, NUM_EPOCHS, PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar LSTM\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì¶ ENTRENANDO MODELO: LSTM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models['lstm'], histories['lstm'] = train_model_keras(\n",
    "    'lstm', vocab_size, train_dataset, val_dataset,\n",
    "    HIDDEN_SIZE, NUM_LAYERS, EMBEDDING_DIM, DROPOUT, EMBED_DROPOUT,\n",
    "    LEARNING_RATE, WEIGHT_DECAY, LABEL_SMOOTHING, NUM_EPOCHS, PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar GRU\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì¶ ENTRENANDO MODELO: GRU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models['gru'], histories['gru'] = train_model_keras(\n",
    "    'gru', vocab_size, train_dataset, val_dataset,\n",
    "    HIDDEN_SIZE, NUM_LAYERS, EMBEDDING_DIM, DROPOUT, EMBED_DROPOUT,\n",
    "    LEARNING_RATE, WEIGHT_DECAY, LABEL_SMOOTHING, NUM_EPOCHS, PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar curvas de entrenamiento\n",
    "colors = {'rnn': '#e74c3c', 'lstm': '#3498db', 'gru': '#2ecc71'}\n",
    "labels_map = {'rnn': 'SimpleRNN', 'lstm': 'LSTM', 'gru': 'GRU'}\n",
    "\n",
    "if histories:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for model_type, history in histories.items():\n",
    "        val_ppl = [np.exp(loss) for loss in history.history['val_loss']]\n",
    "        epochs = range(1, len(val_ppl) + 1)\n",
    "        axes[0].plot(epochs, val_ppl, color=colors[model_type],\n",
    "                     label=labels_map[model_type], linewidth=2, marker='o', markersize=4)\n",
    "    axes[0].set_xlabel('√âpoca')\n",
    "    axes[0].set_ylabel('Perplejidad')\n",
    "    axes[0].set_title('Perplejidad de Validaci√≥n')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for model_type, history in histories.items():\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        axes[1].plot(epochs, history.history['loss'], color=colors[model_type],\n",
    "                     label=f\"{labels_map[model_type]} (train)\", linestyle='-')\n",
    "        axes[1].plot(epochs, history.history['val_loss'], color=colors[model_type],\n",
    "                     label=f\"{labels_map[model_type]} (val)\", linestyle='--')\n",
    "    axes[1].set_xlabel('√âpoca')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Curvas de Aprendizaje')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Resumen de Modelos:\")\n",
    "    print(\"=\" * 55)\n",
    "    for model_type in histories.keys():\n",
    "        best_ppl = np.exp(min(histories[model_type].history['val_loss']))\n",
    "        print(f\"  {labels_map[model_type]:<10}: PPL = {best_ppl:.2f}\")\n",
    "    print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Generaci√≥n de Secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar mejor modelo\n",
    "best_model_type = min(histories, key=lambda x: min(histories[x].history['val_loss']))\n",
    "model = models[best_model_type]\n",
    "best_ppl = np.exp(min(histories[best_model_type].history['val_loss']))\n",
    "print(f\"üèÜ Usando modelo: {best_model_type.upper()} (PPL: {best_ppl:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(model, seed_text, max_length, num_chars):\n",
    "    \"\"\"Genera texto usando b√∫squeda voraz.\"\"\"\n",
    "    generated_text = seed_text.lower()\n",
    "    for _ in range(num_chars):\n",
    "        tokens = [char2idx.get(ch, 0) for ch in generated_text[-max_length:]]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens = [0] * (max_length - len(tokens)) + tokens\n",
    "        x = np.array([tokens], dtype=np.int32)\n",
    "        logits = model.predict(x, verbose=0)\n",
    "        next_char_idx = np.argmax(logits[0, -1, :])\n",
    "        generated_text += idx2char[next_char_idx]\n",
    "    return generated_text\n",
    "\n",
    "print(\"‚úÖ Funci√≥n greedy_search definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(model, seed_text, max_length, num_chars, temperature=1.0):\n",
    "    \"\"\"Genera texto usando muestreo con temperatura.\"\"\"\n",
    "    generated_text = seed_text.lower()\n",
    "    for _ in range(num_chars):\n",
    "        tokens = [char2idx.get(ch, 0) for ch in generated_text[-max_length:]]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens = [0] * (max_length - len(tokens)) + tokens\n",
    "        x = np.array([tokens], dtype=np.int32)\n",
    "        logits = model.predict(x, verbose=0)\n",
    "        logits_scaled = logits[0, -1, :] / temperature\n",
    "        probs = tf.nn.softmax(logits_scaled).numpy()\n",
    "        next_char_idx = np.random.choice(len(probs), p=probs)\n",
    "        generated_text += idx2char[next_char_idx]\n",
    "    return generated_text\n",
    "\n",
    "print(\"‚úÖ Funci√≥n sample_with_temperature definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_deterministic(model, seed_text, max_length, num_chars, beam_width=5):\n",
    "    \"\"\"Genera texto usando beam search determin√≠stico.\"\"\"\n",
    "    seed_text = seed_text.lower()\n",
    "    beams = [(seed_text, 0.0)]\n",
    "    for _ in range(num_chars):\n",
    "        all_candidates = []\n",
    "        for text, score in beams:\n",
    "            tokens = [char2idx.get(ch, 0) for ch in text[-max_length:]]\n",
    "            if len(tokens) < max_length:\n",
    "                tokens = [0] * (max_length - len(tokens)) + tokens\n",
    "            x = np.array([tokens], dtype=np.int32)\n",
    "            logits = model.predict(x, verbose=0)\n",
    "            log_probs = tf.nn.log_softmax(logits[0, -1, :]).numpy()\n",
    "            top_indices = np.argsort(log_probs)[-beam_width:]\n",
    "            for idx in top_indices:\n",
    "                all_candidates.append((text + idx2char[idx], score + log_probs[idx]))\n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = all_candidates[:beam_width]\n",
    "    final_sequences = [(text, score / len(text)) for text, score in beams]\n",
    "    final_sequences.sort(key=lambda x: x[1], reverse=True)\n",
    "    return final_sequences[0][0], final_sequences\n",
    "\n",
    "print(\"‚úÖ Funci√≥n beam_search_deterministic definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_stochastic(model, seed_text, max_length, num_chars, beam_width=5, temperature=1.0):\n",
    "    \"\"\"Genera texto usando beam search estoc√°stico.\"\"\"\n",
    "    seed_text = seed_text.lower()\n",
    "    beams = [(seed_text, 0.0)]\n",
    "    for _ in range(num_chars):\n",
    "        all_candidates = []\n",
    "        for text, score in beams:\n",
    "            tokens = [char2idx.get(ch, 0) for ch in text[-max_length:]]\n",
    "            if len(tokens) < max_length:\n",
    "                tokens = [0] * (max_length - len(tokens)) + tokens\n",
    "            x = np.array([tokens], dtype=np.int32)\n",
    "            logits = model.predict(x, verbose=0)\n",
    "            logits_scaled = logits[0, -1, :] / temperature\n",
    "            probs = tf.nn.softmax(logits_scaled).numpy()\n",
    "            log_probs = np.log(probs + 1e-10)\n",
    "            sampled_indices = np.random.choice(len(probs), size=min(beam_width, len(probs)),\n",
    "                                               replace=False, p=probs)\n",
    "            for idx in sampled_indices:\n",
    "                all_candidates.append((text + idx2char[idx], score + log_probs[idx]))\n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = all_candidates[:beam_width]\n",
    "    final_sequences = [(text, score / len(text)) for text, score in beams]\n",
    "    final_sequences.sort(key=lambda x: x[1], reverse=True)\n",
    "    return final_sequences[0][0], final_sequences\n",
    "\n",
    "print(\"‚úÖ Funci√≥n beam_search_stochastic definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de generaci√≥n\n",
    "seed = \"ulises dijo\"\n",
    "print(\"=\"*70)\n",
    "print(f\"üî¨ COMPARACI√ìN DE M√âTODOS - Semilla: '{seed}'\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìç GREEDY:\")\n",
    "print(greedy_search(model, seed, MAX_CONTEXT_SIZE, 100))\n",
    "\n",
    "print(\"\\nüé≤ SAMPLING (T=0.7):\")\n",
    "print(sample_with_temperature(model, seed, MAX_CONTEXT_SIZE, 100, 0.7))\n",
    "\n",
    "print(\"\\nüìä BEAM SEARCH (width=5):\")\n",
    "result, _ = beam_search_deterministic(model, seed, MAX_CONTEXT_SIZE, 100, 5)\n",
    "print(result)\n",
    "\n",
    "print(\"\\nüéØ BEAM STOCHASTIC (width=5, T=0.7):\")\n",
    "result, _ = beam_search_stochastic(model, seed, MAX_CONTEXT_SIZE, 100, 5, 0.7)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efecto de la temperatura\n",
    "seed = \"el h√©roe regres√≥\"\n",
    "print(\"=\"*70)\n",
    "print(f\"üå°Ô∏è EFECTO DE LA TEMPERATURA - Semilla: '{seed}'\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for temp in [0.2, 0.5, 0.8, 1.0, 1.5]:\n",
    "    print(f\"\\nüå°Ô∏è T = {temp}:\")\n",
    "    print(sample_with_temperature(model, seed, MAX_CONTEXT_SIZE, 80, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo\n",
    "model.save('best_char_lm_keras.keras')\n",
    "print(\"‚úÖ Modelo guardado en 'best_char_lm_keras.keras'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fin del Desaf√≠o 3 (Versi√≥n Keras)\n",
    "\n",
    "‚úÖ **Objetivos cumplidos:**\n",
    "1. Corpus seleccionado y preprocesado\n",
    "2. Tokenizaci√≥n por caracteres implementada\n",
    "3. Tres arquitecturas RNN evaluadas (SimpleRNN, LSTM, GRU)\n",
    "4. Cuatro m√©todos de generaci√≥n implementados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
